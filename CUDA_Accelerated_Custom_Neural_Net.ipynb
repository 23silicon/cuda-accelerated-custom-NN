{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23silicon/deep-learning/blob/main/CUDA_Accelerated_Custom_Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41YP1fsszQmd"
      },
      "outputs": [],
      "source": [
        "# PROJECT OVERVIEW:\n",
        "# A while ago, I built a neural network in python from scratch without any ML libraries that achieves a 97.83% accuracy on MNIST after 1000 epochs.\n",
        "# This project uses a matrix multiplication CUDA kernel I wrote to significantly accelerate both inference and backpropagation by parallelizing on the GPU.\n",
        "# Uses Pybind11 to call the kernel (written in C++) from my network in python.\n",
        "\n",
        "\"\"\"\n",
        "On CPU:                      Training completes in 40m 4.68s\n",
        "On GPU (with matmul kernel): Training completes in 16m 37.23s\n",
        "\n",
        "Speedup: 2.411x\n",
        "\"\"\"\n",
        "\n",
        "# GPU: NVIDIA Tesla T4\n",
        "# Cuda matmul kernels launched in lines 93, 169, 171 in the neural network training cell.\n",
        "# TO DO for much faster training: (Once openml finally works again)\n",
        "  # Implement vector addition kernel for adding result of matmul to bias vectors.\n",
        "  # Replace numpy with torch tensors so copy between device and host is not needed.\n",
        "\n",
        "\n",
        "\n",
        "#!nvidia-smi\n",
        "#!nvcc --version\n",
        "!pip install pybind11\n",
        "!pip install ninja\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=compute_cap --format=csv,noheader,nounits\")\n",
        "gpu_arch = f\"sm_{str.strip(gpu_info).replace('.', '')}\"\n",
        "print(gpu_arch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6akjDIPGQKs",
        "outputId": "dcacdcac-e46e-44f7-a549-8ef5d36607b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_kernel.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_kernel.cu\n",
        "//%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define tilewidth 32\n",
        "\n",
        "extern \"C\" {\n",
        "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    int col = blockDim.x*blockIdx.x+threadIdx.x;\n",
        "    int row = blockDim.y*blockIdx.y+threadIdx.y;\n",
        "\n",
        "    __shared__ float Atile[tilewidth][tilewidth];\n",
        "    __shared__ float Btile[tilewidth][tilewidth];\n",
        "\n",
        "\n",
        "    float value = 0;\n",
        "    for (int t = 0; t < (N+tilewidth-1)/tilewidth; t++) {\n",
        "        int tilecol = t * tilewidth + threadIdx.x;\n",
        "        int tilerow = t * tilewidth + threadIdx.y;\n",
        "        Atile[threadIdx.y][threadIdx.x] = (row < M && tilecol < N) ? A[row*N + tilecol] : 0.0f;\n",
        "        Btile[threadIdx.y][threadIdx.x] = (tilerow < N && col < K) ? B[tilerow*K + col] : 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int k = 0 ; k < tilewidth; k++) {\n",
        "        value += Atile[threadIdx.y][k] * Btile[k][threadIdx.x];\n",
        "    }\n",
        "    __syncthreads();\n",
        "    }\n",
        "    if (row < M && col < K) { //Final matrix is MxK\n",
        "        C[row * K + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "void launch_matmul(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "  dim3 threadsPerBlock(tilewidth, tilewidth);\n",
        "  dim3 blocksPerGrid((K + tilewidth - 1) / tilewidth,\n",
        "                    (M + tilewidth - 1) / tilewidth);\n",
        "  matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
        "  cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "//****************************VEC ADD*************************************\n",
        "//to be implemented in my network later.\n",
        "\n",
        "__global__ void vector_add(const float* A, const float* B, float* C, int N) {\n",
        "    int id = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    for (int i = id; i < N; i += gridDim.x * blockDim.x) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void launch_vecadd(const float* A, const float* B, float* C, int N) {\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vector_add<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, N);\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "} //extern C\n",
        "\n",
        "int main() {\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN_5HfLxA4uH",
        "outputId": "adda2bdc-36ef-43de-c9fe-8414f72db0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_wrapper.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_wrapper.cpp\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// Declaration of the CUDA launcher function with C linkage\n",
        "extern \"C\" {\n",
        "void launch_matmul(const float* A, const float* B, float* C, int M, int N, int K);\n",
        "}\n",
        "\n",
        "// Wrapper function called from Python\n",
        "void matmul(torch::Tensor A, torch::Tensor B, torch::Tensor C, int M, int N, int K) {\n",
        "    // Ensure tensors are contiguous and on CUDA\n",
        "    AT_ASSERTM(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "    AT_ASSERTM(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "    AT_ASSERTM(C.is_cuda(), \"C must be a CUDA tensor\");\n",
        "\n",
        "    const float* A_ptr = A.data_ptr<float>();\n",
        "    const float* B_ptr = B.data_ptr<float>();\n",
        "    float* C_ptr = C.data_ptr<float>();\n",
        "\n",
        "    launch_matmul(A_ptr, B_ptr, C_ptr, M, N, K);\n",
        "}\n",
        "\n",
        "// Bindings to Python\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"matmul\", &matmul, \"Matrix multiplication CUDA kernel\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxF5LQNyBSL_",
        "outputId": "0f1402b7-3246-4ad3-dcd6-1e1c8ac81cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module matmul_cuda_v1, skipping build step...\n",
            "Loading extension module matmul_cuda_v1...\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "matmul_module = load(\n",
        "    name=\"matmul_cuda\",\n",
        "    sources=[\"matmul_wrapper.cpp\", \"matmul_kernel.cu\"],\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTdpDTsrBa47"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\"\"\" code to test whether the matmul kernel works\n",
        "N = 512\n",
        "A_np = np.random.rand(N, N).astype(np.float32)\n",
        "B_np = np.random.rand(N, N).astype(np.float32)\n",
        "\n",
        "A = torch.from_numpy(A_np).cuda()\n",
        "B = torch.from_numpy(B_np).cuda()\n",
        "C = torch.empty((N, N), dtype=torch.float32, device='cuda')\n",
        "\n",
        "matmul_module.matmul(A, B, C, 512,512,512)\n",
        "C_cpu = C.cpu().numpy()\n",
        "print(\"Result shape:\", C_cpu.shape)\n",
        "print(C_cpu) \"\"\"\n",
        "\n",
        "#CUDA MATMUL LAUNCHER\n",
        "def launch_cuda_matmul(A, B):\n",
        "    A,B = torch.from_numpy(A).cuda(),torch.from_numpy(B).cuda()\n",
        "    C = torch.empty((A.shape[0], B.shape[1]), dtype=torch.float32, device='cuda')\n",
        "    matmul_module.matmul(A, B, C, A.shape[0], A.shape[1], B.shape[1])\n",
        "    C = C.cpu().numpy()\n",
        "    return C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdT78Q9yCt9C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "\n",
        "#get mnist\n",
        "print(\"Loading MNIST dataset...\")\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "print(\"Data retrieved.\")\n",
        "X,y = mnist['data'], mnist['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wldFB9MZEh5A",
        "outputId": "b57d45d4-5c5a-49b8-ae75-b93c82b7308b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding data...\n",
            "Splitting data...\n",
            "Training neural network...\n",
            "Epoch 0: Accuracy = 30.59% | LR = 1.2\n",
            "Epoch 10: Accuracy = 74.07% | LR = 1.2\n",
            "Epoch 20: Accuracy = 81.90% | LR = 1.2\n",
            "Epoch 30: Accuracy = 87.71% | LR = 1.2\n",
            "Epoch 40: Accuracy = 91.89% | LR = 1.2\n",
            "Epoch 50: Accuracy = 91.26% | LR = 1.2\n",
            "Epoch 60: Accuracy = 93.10% | LR = 1.2\n",
            "Epoch 70: Accuracy = 93.66% | LR = 1.2\n",
            "Epoch 80: Accuracy = 94.04% | LR = 1.2\n",
            "Epoch 90: Accuracy = 94.33% | LR = 1.2\n",
            "Epoch 100: Accuracy = 94.39% | LR = 1.2\n",
            "Epoch 110: Accuracy = 94.29% | LR = 1.2\n",
            "Epoch 120: Accuracy = 94.91% | LR = 1.2\n",
            "Epoch 130: Accuracy = 95.11% | LR = 1.2\n",
            "Epoch 140: Accuracy = 95.18% | LR = 1.2\n",
            "Epoch 150: Accuracy = 95.35% | LR = 1.2\n",
            "Epoch 160: Accuracy = 95.51% | LR = 1.2\n",
            "Epoch 170: Accuracy = 95.64% | LR = 1.2\n",
            "Epoch 180: Accuracy = 95.79% | LR = 1.2\n",
            "Epoch 190: Accuracy = 95.86% | LR = 1.2\n",
            "Epoch 200: Accuracy = 95.98% | LR = 1.2\n",
            "Epoch 210: Accuracy = 96.07% | LR = 1.2\n",
            "Epoch 220: Accuracy = 96.18% | LR = 1.2\n",
            "Epoch 230: Accuracy = 96.29% | LR = 1.2\n",
            "Epoch 240: Accuracy = 96.35% | LR = 1.2\n",
            "Epoch 250: Accuracy = 96.37% | LR = 1.2\n",
            "Epoch 260: Accuracy = 96.43% | LR = 1.2\n",
            "Epoch 270: Accuracy = 96.50% | LR = 1.2\n",
            "Epoch 280: Accuracy = 96.54% | LR = 1.2\n",
            "Epoch 290: Accuracy = 96.59% | LR = 1.2\n",
            "Epoch 300: Accuracy = 96.64% | LR = 1.2\n",
            "Epoch 310: Accuracy = 96.69% | LR = 1.2\n",
            "Epoch 320: Accuracy = 96.76% | LR = 1.2\n",
            "Epoch 330: Accuracy = 96.82% | LR = 1.2\n",
            "Epoch 340: Accuracy = 96.84% | LR = 1.2\n",
            "Epoch 350: Accuracy = 96.87% | LR = 1.2\n",
            "Epoch 360: Accuracy = 96.89% | LR = 1.2\n",
            "Epoch 370: Accuracy = 96.92% | LR = 1.2\n",
            "Epoch 380: Accuracy = 96.94% | LR = 1.2\n",
            "Epoch 390: Accuracy = 96.92% | LR = 1.2\n",
            "Epoch 400: Accuracy = 96.96% | LR = 1.2\n",
            "Epoch 410: Accuracy = 97.00% | LR = 1.2\n",
            "Epoch 420: Accuracy = 97.04% | LR = 1.2\n",
            "Epoch 430: Accuracy = 97.05% | LR = 1.2\n",
            "Epoch 440: Accuracy = 97.06% | LR = 1.2\n",
            "Epoch 450: Accuracy = 97.11% | LR = 1.2\n",
            "Epoch 460: Accuracy = 97.14% | LR = 1.2\n",
            "Epoch 470: Accuracy = 97.16% | LR = 1.2\n",
            "Epoch 480: Accuracy = 97.18% | LR = 1.2\n",
            "Epoch 490: Accuracy = 97.17% | LR = 1.2\n",
            "Epoch 500: Accuracy = 97.18% | LR = 1.2\n",
            "Epoch 510: Accuracy = 97.22% | LR = 1.2\n",
            "Epoch 520: Accuracy = 97.24% | LR = 1.2\n",
            "Epoch 530: Accuracy = 97.23% | LR = 1.2\n",
            "Epoch 540: Accuracy = 97.23% | LR = 1.2\n",
            "Epoch 550: Accuracy = 97.24% | LR = 1.2\n",
            "Epoch 560: Accuracy = 97.24% | LR = 1.2\n",
            "Epoch 570: Accuracy = 97.25% | LR = 1.2\n",
            "Epoch 580: Accuracy = 97.25% | LR = 1.2\n",
            "Epoch 590: Accuracy = 97.28% | LR = 1.2\n",
            "Epoch 600: Accuracy = 97.31% | LR = 1.2\n",
            "Epoch 610: Accuracy = 97.34% | LR = 1.2\n",
            "Epoch 620: Accuracy = 97.35% | LR = 1.2\n",
            "Epoch 630: Accuracy = 97.36% | LR = 1.2\n",
            "Epoch 640: Accuracy = 97.37% | LR = 1.2\n",
            "Epoch 650: Accuracy = 97.38% | LR = 1.2\n",
            "Epoch 660: Accuracy = 97.38% | LR = 1.2\n",
            "Epoch 670: Accuracy = 97.38% | LR = 1.2\n",
            "Epoch 680: Accuracy = 97.39% | LR = 1.2\n",
            "Epoch 690: Accuracy = 97.40% | LR = 1.2\n",
            "Epoch 700: Accuracy = 97.41% | LR = 1.2\n",
            "Epoch 710: Accuracy = 97.42% | LR = 1.2\n",
            "Epoch 720: Accuracy = 97.43% | LR = 1.2\n",
            "Epoch 730: Accuracy = 97.44% | LR = 1.2\n",
            "Epoch 740: Accuracy = 97.46% | LR = 1.2\n",
            "Epoch 750: Accuracy = 97.47% | LR = 1.2\n",
            "Epoch 760: Accuracy = 97.47% | LR = 1.2\n",
            "Epoch 770: Accuracy = 97.48% | LR = 1.2\n",
            "Epoch 780: Accuracy = 97.50% | LR = 1.2\n",
            "Epoch 790: Accuracy = 97.51% | LR = 1.2\n",
            "Epoch 800: Accuracy = 97.53% | LR = 1.2\n",
            "Epoch 810: Accuracy = 97.54% | LR = 1.2\n",
            "Epoch 820: Accuracy = 97.55% | LR = 1.2\n",
            "Epoch 830: Accuracy = 97.56% | LR = 1.2\n",
            "Epoch 840: Accuracy = 97.56% | LR = 1.2\n",
            "Epoch 850: Accuracy = 97.56% | LR = 1.2\n",
            "Epoch 860: Accuracy = 97.56% | LR = 1.2\n",
            "Epoch 870: Accuracy = 97.56% | LR = 1.2\n",
            "Epoch 880: Accuracy = 97.58% | LR = 1.2\n",
            "Epoch 890: Accuracy = 97.59% | LR = 1.2\n",
            "Epoch 900: Accuracy = 97.59% | LR = 1.2\n",
            "Epoch 910: Accuracy = 97.61% | LR = 1.2\n",
            "Epoch 920: Accuracy = 97.61% | LR = 1.2\n",
            "Epoch 930: Accuracy = 97.61% | LR = 1.2\n",
            "Epoch 940: Accuracy = 97.61% | LR = 1.2\n",
            "Epoch 950: Accuracy = 97.63% | LR = 1.2\n",
            "Epoch 960: Accuracy = 97.64% | LR = 1.2\n",
            "Epoch 970: Accuracy = 97.64% | LR = 1.2\n",
            "Epoch 980: Accuracy = 97.66% | LR = 1.2\n",
            "Epoch 990: Accuracy = 97.66% | LR = 1.2\n",
            "Training completed in 997.2336895465851 seconds.\n",
            "Calculating final accuracy and graphing accuracy and cost histories...\n",
            "First few predictions:\t [0 4 0 2 0 0 2 6 0 4 1 2 7 3 2 2 1 7 2 6 0 3 7 1 4 1 9 7 4 8 7 8 4 0 3]\n",
            "Expected labels:\t [0 4 0 2 0 0 2 6 0 4 1 2 7 3 2 2 1 7 2 6 0 3 7 1 4 1 9 7 4 8 7 8 4 0 3]\n",
            "Accuracy: 97.66%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "On CPU:                      Training completes in 40m 4.68s\n",
        "On GPU (with matmul kernel): Training completes in 16m 37.23s\n",
        "\n",
        "Speedup: 2.411x\n",
        "\"\"\"\n",
        "\n",
        "#GPU: NVIDIA Tesla T4\n",
        "#Cuda matmul kernels launched in lines 93, 169, 171\n",
        "#TO DO: (Once openml finally works again)\n",
        "  # Implement vector addition kernel for adding result of matmul to bias vectors.\n",
        "  # Replace numpy with torch tensors so copy between device and host is not needed.\n",
        "\n",
        "class FNN_classifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "\n",
        "    def initializeParams(self, layerDims):\n",
        "        #initialize weights and biases of the network given array of dimensions of each layer\n",
        "        np.random.seed(42)\n",
        "        for i in range(1,len(layerDims)):\n",
        "            '''\n",
        "                W_i is a 2 dimensional matrix that is initialized to random values with\n",
        "                dimensions n*m where n is the layers in the current dimension and\n",
        "                m is the layers of the previous dimension. Each value is multiplied by 0.01\n",
        "                to avoid exploding gradients.\n",
        "                The shape of the n*m matrix allows it to be multiplied in the current layer in\n",
        "                forward propagation by the result of the previous layer which is an m-dimensional\n",
        "                vector (Matrix mult: n*m x m*1). This results in an n dimensional vector that can then\n",
        "                be taken to the next layer to repeat the process.\n",
        "\n",
        "                b_i is an n dimensional vector of zeros representing biases. biases don't start\n",
        "                with random values like weights do.\n",
        "\n",
        "                Multiplication structure for a 2 by 4 weight metrix to create the linear hypothesis\n",
        "                (m and n here are unrelated to previous m and n):\n",
        "\n",
        "                [a b c d]   [i]   [m]   [o]\n",
        "                [e f g h] X [j] + [n] = [p]\n",
        "                            [k]\n",
        "                            [l]\n",
        "            '''\n",
        "            self.params['W' + str(i)] = (np.random.randn(layerDims[i],layerDims[i-1])*0.1).astype(np.float32)\n",
        "            self.params['b' + str(i)] = np.zeros((layerDims[i],1)).astype(np.float32)\n",
        "\n",
        "\n",
        "        return self.params\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    Activation functions\n",
        "    \"\"\"\n",
        "    #Default value, can be changed\n",
        "    activation = \"relu\"\n",
        "\n",
        "    #functions return the value of the function given input Z and returns Z for caching\n",
        "    def relu(self,Z):\n",
        "        return np.maximum(0,Z), Z\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return 1/(1+np.exp(-Z)), Z\n",
        "\n",
        "    def tanh(self,Z):\n",
        "        return np.tanh(Z), Z\n",
        "\n",
        "    def softmax(self,Z):\n",
        "        \"\"\"\n",
        "        ex. for a 3 element vector:\n",
        "        Z\n",
        "        \"\"\"\n",
        "        zexp = np.exp(Z - np.max(Z,axis=0,keepdims=True)) #Apparently this is a trick that prevents overflow\n",
        "                                    #This will automatically make it so that e^largest value = e^0 = 1\n",
        "                                    #This is also mathematically equivalent to regular softmax\n",
        "        return zexp/np.sum(zexp, axis=0, keepdims = True), Z\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "    def forwardProp(self,X):\n",
        "        X = X.T\n",
        "        #X is the incoming input matrix\n",
        "        A = X\n",
        "        caches = [] #stores tuples of the linear and activation caches for each layer of the network\n",
        "\n",
        "        L = len(self.params)//2 #self.params is twice as long as the number of layers due to W_i and b_i\n",
        "\n",
        "        for i in range(1, L+1):\n",
        "            prevA = A\n",
        "\n",
        "            #Z = Wx + b - \"linear hypothesis\"\n",
        "            Z = launch_cuda_matmul(self.params['W' + str(i)],prevA) + self.params['b' + str(i)]                         # CUDA REPLACEMENT 1 --------------------------------\n",
        "\n",
        "            #Caching important values from linear hypothesis\n",
        "            linearCache = (prevA, self.params['W' + str(i)], self.params['b' + str(i)])\n",
        "\n",
        "            #Caching important values from activation. Activation function returns the\n",
        "            #value of the function itself and the input Z, the result of the linear hypothesis\n",
        "\n",
        "            func = 0\n",
        "            if i == L: #This is the final layer, where softmax is implemented.\n",
        "                func = self.softmax(Z)\n",
        "                #print(f\"final layer reached, softmaxed. {L}\")\n",
        "            elif self.activation == \"relu\":\n",
        "                #print(f\"relu'd{L}\")\n",
        "                func = self.relu(Z)\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                func = self.sigmoid(Z)\n",
        "            elif self.activation == \"tanh\":\n",
        "                func = self.tanh(Z)\n",
        "\n",
        "            A, activationCache = func\n",
        "\n",
        "            cache = (linearCache,activationCache)\n",
        "            caches.append(cache)\n",
        "\n",
        "        return A, caches\n",
        "\n",
        "\n",
        "    #Now that forward propagation is done, the loss function needs to be defined\n",
        "    def crossEntropyLoss(self, yTrue, yPred):\n",
        "        yTrue = yTrue.T\n",
        "        \"\"\"\n",
        "        Computes cost function for general cross entropy loss, used for multiclass classification\n",
        "        \"\"\"\n",
        "        #accounts for floating point inaccuracies, keeps values between 0 and 1\n",
        "        yPred = np.clip(yPred, 1e-15, 1-1e-15)\n",
        "\n",
        "        #returns a vector of the individual sample losses\n",
        "\n",
        "        losses = -np.sum(yTrue*np.log(yPred),axis=0)\n",
        "\n",
        "        #Determines total cost by taking the average of the loss across the entire dataset.\n",
        "        cost = np.mean(losses)\n",
        "        return cost\n",
        "\n",
        "\n",
        "    #Backprop helper function that runs it for a single layer\n",
        "    def stepBack(self, dA, cache, isOutput = False):\n",
        "        #dA is the gradient of the loss with respect to A, dL/dA\n",
        "        #unpacking cache\n",
        "        linearCache, activationCache = cache\n",
        "        prevA, W, b = linearCache\n",
        "        Z = activationCache\n",
        "        m = prevA.shape[1] #gets number of samples\n",
        "\n",
        "\n",
        "        if isOutput:\n",
        "            dZ = dA\n",
        "        else:\n",
        "            if self.activation == \"relu\":\n",
        "                dZ = dA * (Z>0) #Interestingly enough, you can multiply numbers by booleans in python.\n",
        "                #This multiplies dA by the derivative of relu which is 1 if positive and 0 if not.\n",
        "            elif self.activation == \"tanh\":\n",
        "                dZ = dA * (1-np.tanh(Z)**2)\n",
        "                #for y = tanh(x), dy/dx = 1-tanh(x)^2\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                dZ = dA*self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "                #for y = sigmoid(x), dy/dx = sigmoid(x)*(1-sigmoid(x))\n",
        "            else:\n",
        "                raise ValueError(\"Activation function not supported\")\n",
        "\n",
        "        #Now, we can compute gradients for W, b, and prevA\n",
        "        \"\"\"print(\"dZ.shape\", dZ.shape, \"contig?\", dZ.flags['C_CONTIGUOUS'])\n",
        "        print(\"prevA.T.shape\", prevA.T.shape, \"contig?\", prevA.T.flags['C_CONTIGUOUS'])\n",
        "        print(\"W.T.shape\", W.T.shape, \"contig?\", W.T.flags['C_CONTIGUOUS'])\"\"\"\n",
        "\n",
        "        dW = (1/m)*launch_cuda_matmul(np.ascontiguousarray(dZ),np.ascontiguousarray(prevA.T))                              #CUDA REPLACEMENT 2 (88 160 162) -------------------\n",
        "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
        "        prevdA = launch_cuda_matmul(np.ascontiguousarray(W.T), np.ascontiguousarray(dZ))                                   #CUDA REPLACEMENT 3 --------------------------------\n",
        "\n",
        "        return prevdA, dW, db\n",
        "\n",
        "    #Now that the stepback function is defined for each layer going backwards, we can do backprop\n",
        "    def backprop(self, AL, yPred, caches):\n",
        "        grads = {}\n",
        "        L = len(caches)\n",
        "\n",
        "        m = AL.shape[1]\n",
        "        #Remember, AL is the final activations for every sample represented as a matrix of column vectors\n",
        "        '''\n",
        "        ex.      [p0a p0b p0c p0d p0e]\n",
        "                 [p1a p1b p1c p1d p1e]\n",
        "                 [p2a p2b p2c p2d p2e]\n",
        "                 [p3a p3b p3c p3d p3e]\n",
        "                 [p4a p4b p4c p4d p4e]\n",
        "        Where each final letter of a term represents a different sample. AL.shape[1] returns the number of columns,\n",
        "        also the number of samples.\n",
        "        '''\n",
        "        yPred = yPred.T\n",
        "\n",
        "        dAL = AL - yPred\n",
        "        #Gradient of the loss function with respect to AL for general cross entropy loss.\n",
        "\n",
        "        currentCache = caches[L-1]\n",
        "        grads['dA' + str(L-1)], grads['dW' + str(L-1)], grads['db' + str(L-1)] = self.stepBack(dAL, currentCache, isOutput=True)\n",
        "\n",
        "        for i in reversed(range(L-1)):\n",
        "            currentCache = caches[i]\n",
        "            prevdAtemp, dWtemp, dbtemp = self.stepBack(grads['dA' + str(i+1)], currentCache)\n",
        "\n",
        "            grads['dA' + str(i)] = prevdAtemp\n",
        "            #i+1 is used in order to match up the dA gradient used to calculate the weights and biases\n",
        "            #of the following layer.\n",
        "            grads['dW' + str(i)] = dWtemp\n",
        "            grads['db' + str(i)] = dbtemp\n",
        "#        print(grads.keys())\n",
        "        return grads\n",
        "\n",
        "    #Now its time for gradient descent!!! Actually updating the weights and biases according to calculated\n",
        "    #grads and the learning rate, a hyperparameter.\n",
        "    '''\n",
        "    The learning rate hyperparameter exists to facilitate the size of steps in gradient descent.\n",
        "    If the learning rate is too small, convergence to a local minimum takes too long.\n",
        "    If the learning rate is too big, the process can overshoot and diverge, not reaching the minimum.\n",
        "    '''\n",
        "    #Also, we don't know when a minimum is reached, we just try to get close. More epochs exist to try\n",
        "    #and approach the minimum of the cost function.\n",
        "    '''\n",
        "    Many optimizers use dynamic learning rates, decreasing step size as a minimum seems to be approached\n",
        "    to get a more accurate estimate without overshooting.\n",
        "    '''\n",
        "    def updateParameters(self, grads, learningRate):\n",
        "        L = len(self.params) // 2 #layer number\n",
        "\n",
        "#        print(L)\n",
        "        for i in range(L):\n",
        "            #starting with first hidden layer updating current Weights\n",
        "\n",
        "\n",
        "\n",
        "            self.params['W'+str(i+1)] = self.params['W'+str(i+1)] - learningRate*grads['dW'+str(i)]\n",
        "            #then biases\n",
        "            self.params['b'+str(i+1)] = self.params['b'+str(i+1)] - learningRate*grads['db'+str(i)]\n",
        "\n",
        "            '''\n",
        "            W_new = W - lr*grad_b\n",
        "            b_new = b - lr*grad_b\n",
        "\n",
        "            What it looks like to me is a steeper negative gradient will increase a weight ot a bias\n",
        "            while a positive gradient value decreases the weight or the bias.\n",
        "\n",
        "            A positive gradient means that increasing that parameter would increase the cost\n",
        "            (if we consider partial derivative of the cost function with respect to weights or biases),\n",
        "            so we decrease a weight associated with a positive gradient. Negative gradients decrease the cost\n",
        "            so we increase those weights to emphasize them more.\n",
        "            '''\n",
        "\n",
        "    #Now the function must be trained.\n",
        "    X_TEST = None\n",
        "    Y_TEST = None\n",
        "    accHistory = []\n",
        "    lrHist = []\n",
        "    def train(self, X, y, layerDims, epochs, learningRate, runOptimizer = True):\n",
        "\n",
        "        params = self.initializeParams(layerDims)\n",
        "        costHistory = []\n",
        "        accSlopes = []\n",
        "        for i in range(epochs):\n",
        "            Y_hat, caches = self.forwardProp(X)\n",
        "            cost = self.crossEntropyLoss(Y_hat,y)\n",
        "            costHistory.append(cost)\n",
        "            grads = self.backprop(Y_hat, y, caches)\n",
        "\n",
        "            params = self.updateParameters(grads, learningRate)\n",
        "\n",
        "            accuracy = np.mean(self.predict(FNN_classifier.X_TEST) == np.argmax(FNN_classifier.Y_TEST, axis=1))\n",
        "            self.accHistory.append(accuracy)\n",
        "\n",
        "            #Approximates derivative of the accHistory function\n",
        "            if len(self.accHistory) > 1:\n",
        "                accSlopes.append(self.accHistory[i]-self.accHistory[i-1])\n",
        "\n",
        "            #running my optimizer\n",
        "            if runOptimizer:\n",
        "                if len(accSlopes) >= 3:\n",
        "                    learningRate = self.updateLR(i, epochs, accSlopes, learningRate)\n",
        "                    self.lrHist.append(learningRate)\n",
        "\n",
        "            if i%10 == 0:\n",
        "                print(f\"Epoch {i}: Accuracy = {accuracy * 100:.2f}% | LR = {learningRate}\")\n",
        "\n",
        "        return params, costHistory\n",
        "\n",
        "    def predict(self, X):\n",
        "        A, _ = self.forwardProp(X)\n",
        "#        print(A.shape)\n",
        "        return np.argmax(A, axis=0) #return highest probability index\n",
        "\n",
        "    #My self-designed optimizer program inspired from RMS-prop.\n",
        "    def updateLR(self, i, epochs, accSlopes, learningRate):\n",
        "        first = accSlopes[0]\n",
        "        if first < 0:\n",
        "            first = 0 - first\n",
        "        lr = learningRate\n",
        "        ###recentSlopeAvg = (accSlopes[i] + accSlopes[i-1] + accSlopes[i-2])/3\n",
        "        #Optimization begins after certain fraction of allotted epochs is done to make sure stabilization has started.\n",
        "        #For index errors, stops before last epoch.\n",
        "        if epochs > 0 and i/epochs > 0.2 and i < epochs:\n",
        "            if accSlopes[i-1] < accSlopes[i-2] and accSlopes[i-1] > 0:\n",
        "                #decrease the learning rate proportionally to the derivative decrease but within limits 0.75 to 1.\n",
        "                changeFactor = accSlopes[i-1]/first\n",
        "                if changeFactor < 0.75:\n",
        "                    changeFactor = 0.75\n",
        "                if changeFactor > 1:\n",
        "                    changeFactor = 1\n",
        "                lr *= changeFactor\n",
        "            if accSlopes[i-1] < 0:\n",
        "                cslope = 0 - accSlopes[i-1]\n",
        "                changeFactor = max(cslope,first)/min(cslope,first) #ensures >1 value\n",
        "                if changeFactor < 1:\n",
        "                    changeFactor = 1\n",
        "                if changeFactor > 1.1:\n",
        "                    changeFactor = 1.1\n",
        "                lr *= changeFactor\n",
        "\n",
        "        return lr\n",
        "\n",
        "\n",
        "print(\"Encoding data...\")\n",
        "\n",
        "y = y.astype(int)\n",
        "X = X/255.0\n",
        "\n",
        "#one-hot encode labels in y\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(np.array(y).reshape(-1,1)) #y.shape = (70000, 10)\n",
        "#X = X.T #Input format is n*m where m is samples, and current format is 70000*784. Changes to 784*70000\n",
        "\n",
        "\n",
        "print(\"Splitting data...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
        "\n",
        "\n",
        "X_test = X_test.to_numpy().astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "FNN_classifier.X_TEST = X_test\n",
        "FNN_classifier.Y_TEST = y_test\n",
        "\n",
        "\n",
        "\n",
        "layerDims = [784,512,10] #1 hidden layer specified. 784*256 + 256*10 = 200704 + 2560 = 203264 total weights\n",
        "\n",
        "\n",
        "print(\"Training neural network...\")\n",
        "\n",
        "network = FNN_classifier()\n",
        "network.activation = \"relu\" #relu, tanh, sigmoid are available\n",
        "\n",
        "\n",
        "X_train = X_train.to_numpy().astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "params, costHistory = network.train(X_train, y_train, layerDims, epochs = 1000, learningRate = 1.2, runOptimizer=False)\n",
        "#use 0.6 learning rate for fashion mnist, 1.2 for mnist\n",
        "end = time.time() #times training\n",
        "print(f\"Training completed in {end - start} seconds.\")\n",
        "#Make plots to display mnist numbers and plot cost history\n",
        "\n",
        "\n",
        "\n",
        "print(\"Calculating final accuracy and graphing accuracy and cost histories...\")\n",
        "yPred = network.predict(X_test)\n",
        "\n",
        "#    print(\"Unique classes predicted:\", np.unique(yPred))\n",
        "print(\"First few predictions:\\t\", yPred[:35])\n",
        "print(\"Expected labels:\\t\", np.argmax(y_test, axis=1)[:35])\n",
        "#    print(yPred.shape)\n",
        "#    print(y_test.shape)\n",
        "accuracy = np.mean(yPred == np.argmax(y_test, axis=1))\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "fig,axs = plt.subplots(3)\n",
        "fig.suptitle('Network Accuracy, Cost Histories and LR History')\n",
        "axs[0].plot(network.accHistory, label = 'Accuracy History', color = 'blue')\n",
        "axs[1].plot(costHistory, label = 'Cost History', color = 'red')\n",
        "#axs[2].plot(network.lrHist, label = 'LR History', color = 'green')\n",
        "fig.legend()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(10)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3a9acbe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Test with some sample matrices\n",
        "M, N, K = 64, 128, 32 # Choose some dimensions for testing\n",
        "\n",
        "A_np = np.random.rand(M, N).astype(np.float32)\n",
        "B_np = np.random.rand(N, K).astype(np.float32)\n",
        "\n",
        "# Perform matrix multiplication using numpy\n",
        "C_np = np.dot(A_np, B_np)\n",
        "\n",
        "# Perform matrix multiplication using your CUDA kernel\n",
        "A_cuda = torch.from_numpy(A_np).cuda()\n",
        "B_cuda = torch.from_numpy(B_np).cuda()\n",
        "C_cuda = torch.empty((M, K), dtype=torch.float32, device='cuda')\n",
        "\n",
        "matmul_module.matmul(A_cuda, B_cuda, C_cuda, M, N, K)\n",
        "C_cuda_np = C_cuda.cpu().numpy()\n",
        "\n",
        "# Compare the results\n",
        "tolerance = 1e-5\n",
        "are_close = np.allclose(C_np, C_cuda_np, atol=tolerance)\n",
        "\n",
        "print(f\"Numpy result shape: {C_np.shape}\")\n",
        "print(f\"CUDA kernel result shape: {C_cuda_np.shape}\")\n",
        "print(f\"Are the results from NumPy and CUDA kernel close? {are_close}\")\n",
        "\n",
        "if not are_close:\n",
        "    print(\"\\nDiscrepancies found. Let's look at a small section of the results:\")\n",
        "    print(\"NumPy result (first 5x5):\\n\", C_np[:5, :5])\n",
        "    print(\"CUDA kernel result (first 5x5):\\n\", C_cuda_np[:5, :5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "class FNN_classifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "\n",
        "    def initializeParams(self, layerDims):\n",
        "        #initialize weights and biases of the network given array of dimensions of each layer\n",
        "        np.random.seed(42)\n",
        "        for i in range(1,len(layerDims)):\n",
        "            '''\n",
        "                W_i is a 2 dimensional matrix that is initialized to random values with\n",
        "                dimensions n*m where n is the layers in the current dimension and\n",
        "                m is the layers of the previous dimension. Each value is multiplied by 0.01\n",
        "                to avoid exploding gradients.\n",
        "                The shape of the n*m matrix allows it to be multiplied in the current layer in\n",
        "                forward propagation by the result of the previous layer which is an m-dimensional\n",
        "                vector (Matrix mult: n*m x m*1). This results in an n dimensional vector that can then\n",
        "                be taken to the next layer to repeat the process.\n",
        "\n",
        "                b_i is an n dimensional vector of zeros representing biases. biases don't start\n",
        "                with random values like weights do.\n",
        "\n",
        "                Multiplication structure for a 2 by 4 weight metrix to create the linear hypothesis\n",
        "                (m and n here are unrelated to previous m and n):\n",
        "\n",
        "                [a b c d]   [i]   [m]   [o]\n",
        "                [e f g h] X [j] + [n] = [p]\n",
        "                            [k]\n",
        "                            [l]\n",
        "            '''\n",
        "            self.params['W' + str(i)] = np.random.randn(layerDims[i],layerDims[i-1])*0.1\n",
        "            self.params['b' + str(i)] = np.zeros((layerDims[i],1))\n",
        "\n",
        "\n",
        "        return self.params\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    Activation functions\n",
        "    \"\"\"\n",
        "    #Default value, can be changed\n",
        "    activation = \"relu\"\n",
        "\n",
        "    #functions return the value of the function given input Z and returns Z for caching\n",
        "    def relu(self,Z):\n",
        "        return np.maximum(0,Z), Z\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return 1/(1+np.exp(-Z)), Z\n",
        "\n",
        "    def tanh(self,Z):\n",
        "        return np.tanh(Z), Z\n",
        "\n",
        "    def softmax(self,Z):\n",
        "        \"\"\"\n",
        "        ex. for a 3 element vector:\n",
        "        Z\n",
        "        \"\"\"\n",
        "        zexp = np.exp(Z - np.max(Z,axis=0,keepdims=True)) #Apparently this is a trick that prevents overflow\n",
        "                                    #This will automatically make it so that e^largest value = e^0 = 1\n",
        "                                    #This is also mathematically equivalent to regular softmax\n",
        "        return zexp/np.sum(zexp, axis=0, keepdims = True), Z\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "    def forwardProp(self,X):\n",
        "        X = X.T\n",
        "        #X is the incoming input matrix\n",
        "        A = X\n",
        "        caches = [] #stores tuples of the linear and activation caches for each layer of the network\n",
        "\n",
        "        L = len(self.params)//2 #self.params is twice as long as the number of layers due to W_i and b_i\n",
        "\n",
        "        for i in range(1, L+1):\n",
        "            prevA = A\n",
        "\n",
        "            #Z = Wx + b - \"linear hypothesis\"\n",
        "\n",
        "            Z = np.dot(self.params['W' + str(i)],prevA) + self.params['b' + str(i)]\n",
        "\n",
        "            #Caching important values from linear hypothesis\n",
        "            linearCache = (prevA, self.params['W' + str(i)], self.params['b' + str(i)])\n",
        "\n",
        "            #Caching important values from activation. Activation function returns the\n",
        "            #value of the function itself and the input Z, the result of the linear hypothesis\n",
        "\n",
        "            func = 0\n",
        "            if i == L: #This is the final layer, where softmax is implemented.\n",
        "                func = self.softmax(Z)\n",
        "                #print(f\"final layer reached, softmaxed. {L}\")\n",
        "            elif self.activation == \"relu\":\n",
        "                #print(f\"relu'd{L}\")\n",
        "                func = self.relu(Z)\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                func = self.sigmoid(Z)\n",
        "            elif self.activation == \"tanh\":\n",
        "                func = self.tanh(Z)\n",
        "\n",
        "            A, activationCache = func\n",
        "\n",
        "            cache = (linearCache,activationCache)\n",
        "            caches.append(cache)\n",
        "\n",
        "        return A, caches\n",
        "\n",
        "\n",
        "    #Now that forward propagation is done, the loss function needs to be defined\n",
        "    def crossEntropyLoss(self, yTrue, yPred):\n",
        "        yTrue = yTrue.T\n",
        "        \"\"\"\n",
        "        Computes cost function for general cross entropy loss, used for multiclass classification\n",
        "        \"\"\"\n",
        "        #accounts for floating point inaccuracies, keeps values between 0 and 1\n",
        "        yPred = np.clip(yPred, 1e-15, 1-1e-15)\n",
        "\n",
        "        #returns a vector of the individual sample losses\n",
        "\n",
        "        losses = -np.sum(yTrue*np.log(yPred),axis=0)\n",
        "\n",
        "        #Determines total cost by taking the average of the loss across the entire dataset.\n",
        "        cost = np.mean(losses)\n",
        "        return cost\n",
        "\n",
        "\n",
        "    #Backprop helper function that runs it for a single layer\n",
        "    def stepBack(self, dA, cache, isOutput = False):\n",
        "        #dA is the gradient of the loss with respect to A, dL/dA\n",
        "        #unpacking cache\n",
        "        linearCache, activationCache = cache\n",
        "        prevA, W, b = linearCache\n",
        "        Z = activationCache\n",
        "        m = prevA.shape[1] #gets number of samples\n",
        "\n",
        "\n",
        "        if isOutput:\n",
        "            dZ = dA\n",
        "        else:\n",
        "            if self.activation == \"relu\":\n",
        "                dZ = dA * (Z>0) #Interestingly enough, you can multiply numbers by booleans in python.\n",
        "                #This multiplies dA by the derivative of relu which is 1 if positive and 0 if not.\n",
        "            elif self.activation == \"tanh\":\n",
        "                dZ = dA * (1-np.tanh(Z)**2)\n",
        "                #for y = tanh(x), dy/dx = 1-tanh(x)^2\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                dZ = dA*self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "                #for y = sigmoid(x), dy/dx = sigmoid(x)*(1-sigmoid(x))\n",
        "            else:\n",
        "                raise ValueError(\"Activation function not supported\")\n",
        "\n",
        "        #Now, we can compute gradients for W, b, and prevA\n",
        "        dW = (1/m)*np.dot(dZ,prevA.T)\n",
        "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
        "        prevdA = np.dot(W.T, dZ)\n",
        "\n",
        "        return prevdA, dW, db\n",
        "\n",
        "    #Now that the stepback function is defined for each layer going backwards, we can do backprop\n",
        "    def backprop(self, AL, yPred, caches):\n",
        "        grads = {}\n",
        "        L = len(caches)\n",
        "\n",
        "        m = AL.shape[1]\n",
        "        #Remember, AL is the final activations for every sample represented as a matrix of column vectors\n",
        "        '''\n",
        "        ex.      [p0a p0b p0c p0d p0e]\n",
        "                 [p1a p1b p1c p1d p1e]\n",
        "                 [p2a p2b p2c p2d p2e]\n",
        "                 [p3a p3b p3c p3d p3e]\n",
        "                 [p4a p4b p4c p4d p4e]\n",
        "        Where each final letter of a term represents a different sample. AL.shape[1] returns the number of columns,\n",
        "        also the number of samples.\n",
        "        '''\n",
        "        yPred = yPred.T\n",
        "\n",
        "        dAL = AL - yPred\n",
        "        #Gradient of the loss function with respect to AL for general cross entropy loss.\n",
        "\n",
        "        currentCache = caches[L-1]\n",
        "        grads['dA' + str(L-1)], grads['dW' + str(L-1)], grads['db' + str(L-1)] = self.stepBack(dAL, currentCache, isOutput=True)\n",
        "\n",
        "        for i in reversed(range(L-1)):\n",
        "            currentCache = caches[i]\n",
        "            prevdAtemp, dWtemp, dbtemp = self.stepBack(grads['dA' + str(i+1)], currentCache)\n",
        "\n",
        "            grads['dA' + str(i)] = prevdAtemp\n",
        "            #i+1 is used in order to match up the dA gradient used to calculate the weights and biases\n",
        "            #of the following layer.\n",
        "            grads['dW' + str(i)] = dWtemp\n",
        "            grads['db' + str(i)] = dbtemp\n",
        "#        print(grads.keys())\n",
        "        return grads\n",
        "\n",
        "    #Now its time for gradient descent!!! Actually updating the weights and biases according to calculated\n",
        "    #grads and the learning rate, a hyperparameter.\n",
        "    '''\n",
        "    The learning rate hyperparameter exists to facilitate the size of steps in gradient descent.\n",
        "    If the learning rate is too small, convergence to a local minimum takes too long.\n",
        "    If the learning rate is too big, the process can overshoot and diverge, not reaching the minimum.\n",
        "    '''\n",
        "    #Also, we don't know when a minimum is reached, we just try to get close. More epochs exist to try\n",
        "    #and approach the minimum of the cost function.\n",
        "    '''\n",
        "    Many optimizers use dynamic learning rates, decreasing step size as a minimum seems to be approached\n",
        "    to get a more accurate estimate without overshooting.\n",
        "    '''\n",
        "    def updateParameters(self, grads, learningRate):\n",
        "        L = len(self.params) // 2 #layer number\n",
        "\n",
        "#        print(L)\n",
        "        for i in range(L):\n",
        "            #starting with first hidden layer updating current Weights\n",
        "\n",
        "\n",
        "\n",
        "            self.params['W'+str(i+1)] = self.params['W'+str(i+1)] - learningRate*grads['dW'+str(i)]\n",
        "            #then biases\n",
        "            self.params['b'+str(i+1)] = self.params['b'+str(i+1)] - learningRate*grads['db'+str(i)]\n",
        "\n",
        "            '''\n",
        "            W_new = W - lr*grad_b\n",
        "            b_new = b - lr*grad_b\n",
        "\n",
        "            What it looks like to me is a steeper negative gradient will increase a weight ot a bias\n",
        "            while a positive gradient value decreases the weight or the bias.\n",
        "\n",
        "            A positive gradient means that increasing that parameter would increase the cost\n",
        "            (if we consider partial derivative of the cost function with respect to weights or biases),\n",
        "            so we decrease a weight associated with a positive gradient. Negative gradients decrease the cost\n",
        "            so we increase those weights to emphasize them more.\n",
        "            '''\n",
        "\n",
        "    #Now the function must be trained.\n",
        "    X_TEST = None\n",
        "    Y_TEST = None\n",
        "    accHistory = []\n",
        "    lrHist = []\n",
        "    def train(self, X, y, layerDims, epochs, learningRate, runOptimizer = True):\n",
        "\n",
        "        params = self.initializeParams(layerDims)\n",
        "        costHistory = []\n",
        "        accSlopes = []\n",
        "        for i in range(epochs):\n",
        "            Y_hat, caches = self.forwardProp(X)\n",
        "            cost = self.crossEntropyLoss(Y_hat,y)\n",
        "            costHistory.append(cost)\n",
        "            grads = self.backprop(Y_hat, y, caches)\n",
        "\n",
        "            params = self.updateParameters(grads, learningRate)\n",
        "\n",
        "            accuracy = np.mean(self.predict(FNN_classifier.X_TEST) == np.argmax(FNN_classifier.Y_TEST, axis=1))\n",
        "            self.accHistory.append(accuracy)\n",
        "\n",
        "            #Approximates derivative of the accHistory function\n",
        "            if len(self.accHistory) > 1:\n",
        "                accSlopes.append(self.accHistory[i]-self.accHistory[i-1])\n",
        "\n",
        "            #running my optimizer\n",
        "            if runOptimizer:\n",
        "                if len(accSlopes) >= 3:\n",
        "                    learningRate = self.updateLR(i, epochs, accSlopes, learningRate)\n",
        "                    self.lrHist.append(learningRate)\n",
        "\n",
        "            if i%2 == 0:\n",
        "                print(f\"Epoch {i}: Accuracy = {accuracy * 100:.2f}% | LR = {learningRate}\")\n",
        "\n",
        "        return params, costHistory\n",
        "\n",
        "    def predict(self, X):\n",
        "        A, _ = self.forwardProp(X)\n",
        "#        print(A.shape)\n",
        "        return np.argmax(A, axis=0) #return highest probability index\n",
        "\n",
        "    #My self-designed optimizer program inspired from RMS-prop.\n",
        "    def updateLR(self, i, epochs, accSlopes, learningRate):\n",
        "        first = accSlopes[0]\n",
        "        if first < 0:\n",
        "            first = 0 - first\n",
        "        lr = learningRate\n",
        "        ###recentSlopeAvg = (accSlopes[i] + accSlopes[i-1] + accSlopes[i-2])/3\n",
        "        #Optimization begins after certain fraction of allotted epochs is done to make sure stabilization has started.\n",
        "        #For index errors, stops before last epoch.\n",
        "        if epochs > 0 and i/epochs > 0.2 and i < epochs:\n",
        "            if accSlopes[i-1] < accSlopes[i-2] and accSlopes[i-1] > 0:\n",
        "                #decrease the learning rate proportionally to the derivative decrease but within limits 0.75 to 1.\n",
        "                changeFactor = accSlopes[i-1]/first\n",
        "                if changeFactor < 0.75:\n",
        "                    changeFactor = 0.75\n",
        "                if changeFactor > 1:\n",
        "                    changeFactor = 1\n",
        "                lr *= changeFactor\n",
        "            if accSlopes[i-1] < 0:\n",
        "                cslope = 0 - accSlopes[i-1]\n",
        "                changeFactor = max(cslope,first)/min(cslope,first) #ensures >1 value\n",
        "                if changeFactor < 1:\n",
        "                    changeFactor = 1\n",
        "                if changeFactor > 1.1:\n",
        "                    changeFactor = 1.1\n",
        "                lr *= changeFactor\n",
        "\n",
        "        return lr\n",
        "\n",
        "#get mnist\n",
        "print(\"Loading MNIST dataset...\")\n",
        "mnist = fetch_openml('Fashion-MNIST', version=1)\n",
        "X,y = mnist['data'], mnist['target']\n",
        "\n",
        "print(\"Encoding data...\")\n",
        "\n",
        "y = y.astype(int)\n",
        "X = X/255.0\n",
        "\n",
        "#one-hot encode labels in y\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(np.array(y).reshape(-1,1)) #y.shape = (70000, 10)\n",
        "#X = X.T #Input format is n*m where m is samples, and current format is 70000*784. Changes to 784*70000\n",
        "\n",
        "\n",
        "print(\"Splitting data...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
        "\n",
        "FNN_classifier.X_TEST = X_test\n",
        "FNN_classifier.Y_TEST = y_test\n",
        "\n",
        "\n",
        "\n",
        "layerDims = [784,512,10] #1 hidden layer specified. 784*256 + 256*10 = 200704 + 2560 = 203264 total weights\n",
        "\n",
        "\n",
        "print(\"Training neural network...\")\n",
        "\n",
        "network = FNN_classifier()\n",
        "network.activation = \"relu\" #relu, tanh, sigmoid are available\n",
        "\n",
        "start = time.time()\n",
        "params, costHistory = network.train(X_train, y_train, layerDims, epochs = 1000, learningRate = 0.6, runOptimizer=False)\n",
        "#use 0.6 learning rate for fashion mnist, 1.2 for mnist\n",
        "end = time.time() #times training\n",
        "print(f\"Training completed in {end - start} seconds.\")\n",
        "#Make plots to display mnist numbers and plot cost history\n",
        "\n",
        "\n",
        "\n",
        "print(\"Calculating final accuracy and graphing accuracy and cost histories...\")\n",
        "yPred = network.predict(X_test)\n",
        "\n",
        "#    print(\"Unique classes predicted:\", np.unique(yPred))\n",
        "print(\"First few predictions:\\t\", yPred[:35])\n",
        "print(\"Expected labels:\\t\", np.argmax(y_test, axis=1)[:35])\n",
        "#    print(yPred.shape)\n",
        "#    print(y_test.shape)\n",
        "accuracy = np.mean(yPred == np.argmax(y_test, axis=1))\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "fig,axs = plt.subplots(3)\n",
        "fig.suptitle('Network Accuracy, Cost Histories and LR History')\n",
        "axs[0].plot(network.accHistory, label = 'Accuracy History', color = 'blue')\n",
        "axs[1].plot(costHistory, label = 'Cost History', color = 'red')\n",
        "axs[2].plot(network.lrHist, label = 'LR History', color = 'green')\n",
        "fig.legend()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(10)\n",
        "\n",
        "#display mnist digits - WIP\n",
        "'''for i in range(5):\n",
        "    import matplotlib.pyplot as plt2\n",
        "    fig2, axs2 = plt2.subplots(5)\n",
        "    plt2.imshow(X_test.to_numpy()[i].reshape(28,28), cmap='gray')\n",
        "    plt2.title(f'Label: {y_test[i]}')\n",
        "'''\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Zrdb_eVpqqhH",
        "outputId": "9212fa7e-3838-4d39-cd4a-a56ee72c2252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:110: UserWarning: A network error occurred while downloading https://api.openml.org/data/v1/download/18238735. Retrying...\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1218140106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;31m#get mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading MNIST dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fashion-MNIST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;31m# obtain the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATA_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m     bunch = _download_data_to_bunch(\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mreturn_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mno_retry_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     X, y, frame, categories = _retry_with_clean_cache(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_retry_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_arff_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0moutput_array_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \"\"\"\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0mgzip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_openml_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_retries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_open_openml_url\u001b[0;34m(openml_path, data_home, n_retries, delay)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 with closing(\n\u001b[0;32m--> 173\u001b[0;31m                     _retry_on_network_error(n_retries, delay, req.full_url)(urlopen)(\n\u001b[0m\u001b[1;32m    174\u001b[0m                         \u001b[0mreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;31m# 412 is a specific OpenML error code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjN4wQ8J2YOwxztglegAg4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}