{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23silicon/cuda-accelerated-custom-NN/blob/main/CUDA_Accelerated_Custom_Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41YP1fsszQmd",
        "outputId": "b82fd4f7-54ba-4a2e-9ad4-dee464e41998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n",
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpr4gvv5gp\".\n"
          ]
        }
      ],
      "source": [
        "# PROJECT OVERVIEW:\n",
        "# A while ago, I built a neural network in python from scratch without any ML libraries that achieves a 97.83% accuracy on MNIST after 1000 epochs.\n",
        "# This project uses a matrix multiplication CUDA kernel I wrote to significantly accelerate both inference and backpropagation by parallelizing on the GPU.\n",
        "# Uses Pybind11 to call the kernel (written in C++) from my network in python.\n",
        "\n",
        "\"\"\"\n",
        "On CPU:                      Training completes in 40m 4.68s\n",
        "On GPU (with matmul kernel): Training completes in 3m 49.59s\n",
        "\n",
        "Avg speedup: 10.47x faster (best result: 10.57x)\n",
        "\"\"\"\n",
        "\n",
        "# GPU: NVIDIA Tesla T4\n",
        "# Cuda matmul kernels launched in lines 93, 169, 171 in the neural network training cell.\n",
        "# TO DO for much faster training: (Once openml finally works again)\n",
        "  # Implement vector addition kernel for adding result of matmul to bias vectors.\n",
        "  # Replace numpy with torch tensors so copy between device and host is not needed.\n",
        "\n",
        "\n",
        "\n",
        "#!nvidia-smi\n",
        "#!nvcc --version\n",
        "!pip install pybind11\n",
        "!pip install ninja\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMvWwQG2FTsB",
        "outputId": "1c4ce27b-402b-48a8-8bf0-5848e376fc32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sm_75\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=compute_cap --format=csv,noheader,nounits\")\n",
        "gpu_arch = f\"sm_{str.strip(gpu_info).replace('.', '')}\"\n",
        "print(gpu_arch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6akjDIPGQKs",
        "outputId": "24fcdfce-25f5-4814-e460-e1a1cb920c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_kernel.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_kernel.cu\n",
        "//%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define tilewidth 32\n",
        "\n",
        "extern \"C\" {\n",
        "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "    int col = blockDim.x*blockIdx.x+threadIdx.x;\n",
        "    int row = blockDim.y*blockIdx.y+threadIdx.y;\n",
        "\n",
        "    __shared__ float Atile[tilewidth][tilewidth];\n",
        "    __shared__ float Btile[tilewidth][tilewidth];\n",
        "\n",
        "\n",
        "    float value = 0;\n",
        "    for (int t = 0; t < (N+tilewidth-1)/tilewidth; t++) {\n",
        "        int tilecol = t * tilewidth + threadIdx.x;\n",
        "        int tilerow = t * tilewidth + threadIdx.y;\n",
        "        Atile[threadIdx.y][threadIdx.x] = (row < M && tilecol < N) ? A[row*N + tilecol] : 0.0f;\n",
        "        Btile[threadIdx.y][threadIdx.x] = (tilerow < N && col < K) ? B[tilerow*K + col] : 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int k = 0 ; k < tilewidth; k++) {\n",
        "        value += Atile[threadIdx.y][k] * Btile[k][threadIdx.x];\n",
        "    }\n",
        "    __syncthreads();\n",
        "    }\n",
        "    if (row < M && col < K) { //Final matrix is MxK\n",
        "        C[row * K + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "void launch_matmul(const float* A, const float* B, float* C, int M, int N, int K) {\n",
        "  dim3 threadsPerBlock(tilewidth, tilewidth);\n",
        "  dim3 blocksPerGrid((K + tilewidth - 1) / tilewidth,\n",
        "                    (M + tilewidth - 1) / tilewidth);\n",
        "  printf(\"launching\\n\"); // Added newline for better visibility\n",
        "  matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
        "\n",
        "  // Check for kernel launch errors\n",
        "  cudaError_t lastError = cudaGetLastError();\n",
        "  if (lastError != cudaSuccess) {\n",
        "      printf(\"CUDA Kernel Launch failed: %s\\n\", cudaGetErrorString(lastError));\n",
        "  }\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors during synchronization\n",
        "  lastError = cudaGetLastError();\n",
        "  if (lastError != cudaSuccess) {\n",
        "      printf(\"CUDA Device Synchronization failed: %s\\n\", cudaGetErrorString(lastError));\n",
        "  }\n",
        "}\n",
        "\n",
        "//****************************VEC ADD*************************************\n",
        "//Currently just a prototype. Since the network processes all 60000 samples at once, this needs to be modified to support that.\n",
        "\n",
        "__global__ void vector_add(const float* A, const float* B, float* C, int N) {\n",
        "    int id = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    for (int i = id; i < N; i += gridDim.x * blockDim.x) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "void launch_vecadd(const float* A, const float* B, float* C, int N) {\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vector_add<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, N);\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "} //extern C\n",
        "\n",
        "int main() {\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN_5HfLxA4uH",
        "outputId": "f045fbae-47cb-4176-a299-e057675bd3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_wrapper.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_wrapper.cpp\n",
        "#include <torch/extension.h>\n",
        "\n",
        "// Declaration of the CUDA launcher function with C linkage\n",
        "extern \"C\" {\n",
        "void launch_matmul(const float* A, const float* B, float* C, int M, int N, int K);\n",
        "}\n",
        "\n",
        "// Wrapper function called from Python\n",
        "void matmul(torch::Tensor A, torch::Tensor B, torch::Tensor C, int M, int N, int K) {\n",
        "    AT_ASSERTM(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "    AT_ASSERTM(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "    AT_ASSERTM(C.is_cuda(), \"C must be a CUDA tensor\");\n",
        "\n",
        "    const float* A_ptr = A.data_ptr<float>();\n",
        "    const float* B_ptr = B.data_ptr<float>();\n",
        "    float* C_ptr = C.data_ptr<float>();\n",
        "\n",
        "    launch_matmul(A_ptr, B_ptr, C_ptr, M, N, K);\n",
        "}\n",
        "\n",
        "// Bindings to Python\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"matmul\", &matmul, \"Matrix multiplication CUDA kernel\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxF5LQNyBSL_",
        "outputId": "365f74f5-f9ca-4fd7-a78d-cb3678305066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "matmul_module = load(\n",
        "    name=\"matmul_cuda\",\n",
        "    sources=[\"matmul_wrapper.cpp\", \"matmul_kernel.cu\"],\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KTdpDTsrBa47"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\"\"\" code to test whether the matmul kernel works\n",
        "N = 512\n",
        "A_np = np.random.rand(N, N).astype(np.float32)\n",
        "B_np = np.random.rand(N, N).astype(np.float32)\n",
        "\n",
        "A = torch.from_numpy(A_np).cuda()\n",
        "B = torch.from_numpy(B_np).cuda()\n",
        "C = torch.empty((N, N), dtype=torch.float32, device='cuda')\n",
        "\n",
        "matmul_module.matmul(A, B, C, 512,512,512)\n",
        "C_cpu = C.cpu().numpy()\n",
        "print(\"Result shape:\", C_cpu.shape)\n",
        "print(C_cpu) \"\"\"\n",
        "\n",
        "#CUDA MATMUL LAUNCHER\n",
        "def launch_cuda_matmul(A, B):\n",
        "    assert A.is_cuda and B.is_cuda, \"Inputs must be CUDA tensors\"\n",
        "    assert A.is_contiguous() and B.is_contiguous(), \"Inputs must be contiguous tensors\"\n",
        "\n",
        "    C = torch.empty((A.shape[0], B.shape[1]), dtype=torch.float32, device='cuda')\n",
        "\n",
        "    A_ptr = A.data_ptr()\n",
        "    B_ptr = B.data_ptr()\n",
        "    C_ptr = C.data_ptr()\n",
        "\n",
        "    matmul_module.matmul(A, B, C, A.shape[0], A.shape[1], B.shape[1])\n",
        "\n",
        "    return C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wldFB9MZEh5A",
        "outputId": "fb448bde-2b9b-4294-b3ca-35a04f4f8037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "Data loaded and preprocessed.\n",
            "Training neural network...\n",
            "Epoch 0: Accuracy = 27.82% | LR = 1.2\n",
            "Epoch 10: Accuracy = 66.96% | LR = 1.2\n",
            "Epoch 20: Accuracy = 84.65% | LR = 1.2\n",
            "Epoch 30: Accuracy = 89.12% | LR = 1.2\n",
            "Epoch 40: Accuracy = 92.81% | LR = 1.2\n",
            "Epoch 50: Accuracy = 93.07% | LR = 1.2\n",
            "Epoch 60: Accuracy = 93.32% | LR = 1.2\n",
            "Epoch 70: Accuracy = 94.29% | LR = 1.2\n",
            "Epoch 80: Accuracy = 94.69% | LR = 1.2\n",
            "Epoch 90: Accuracy = 94.93% | LR = 1.2\n",
            "Epoch 100: Accuracy = 95.09% | LR = 1.2\n",
            "Epoch 110: Accuracy = 95.37% | LR = 1.2\n",
            "Epoch 120: Accuracy = 95.45% | LR = 1.2\n",
            "Epoch 130: Accuracy = 95.62% | LR = 1.2\n",
            "Epoch 140: Accuracy = 95.78% | LR = 1.2\n",
            "Epoch 150: Accuracy = 95.88% | LR = 1.2\n",
            "Epoch 160: Accuracy = 96.01% | LR = 1.2\n",
            "Epoch 170: Accuracy = 96.15% | LR = 1.2\n",
            "Epoch 180: Accuracy = 96.22% | LR = 1.2\n",
            "Epoch 190: Accuracy = 96.26% | LR = 1.2\n",
            "Epoch 200: Accuracy = 96.35% | LR = 1.2\n",
            "Epoch 210: Accuracy = 96.45% | LR = 1.2\n",
            "Epoch 220: Accuracy = 96.51% | LR = 1.2\n",
            "Epoch 230: Accuracy = 96.56% | LR = 1.2\n",
            "Epoch 240: Accuracy = 96.61% | LR = 1.2\n",
            "Epoch 250: Accuracy = 96.68% | LR = 1.2\n",
            "Epoch 260: Accuracy = 96.73% | LR = 1.2\n",
            "Epoch 270: Accuracy = 96.77% | LR = 1.2\n",
            "Epoch 280: Accuracy = 96.80% | LR = 1.2\n",
            "Epoch 290: Accuracy = 96.84% | LR = 1.2\n",
            "Epoch 300: Accuracy = 96.92% | LR = 1.2\n",
            "Epoch 310: Accuracy = 96.96% | LR = 1.2\n",
            "Epoch 320: Accuracy = 96.97% | LR = 1.2\n",
            "Epoch 330: Accuracy = 97.02% | LR = 1.2\n",
            "Epoch 340: Accuracy = 97.06% | LR = 1.2\n",
            "Epoch 350: Accuracy = 97.12% | LR = 1.2\n",
            "Epoch 360: Accuracy = 97.11% | LR = 1.2\n",
            "Epoch 370: Accuracy = 97.14% | LR = 1.2\n",
            "Epoch 380: Accuracy = 97.17% | LR = 1.2\n",
            "Epoch 390: Accuracy = 97.20% | LR = 1.2\n",
            "Epoch 400: Accuracy = 97.25% | LR = 1.2\n",
            "Epoch 410: Accuracy = 97.27% | LR = 1.2\n",
            "Epoch 420: Accuracy = 97.28% | LR = 1.2\n",
            "Epoch 430: Accuracy = 97.30% | LR = 1.2\n",
            "Epoch 440: Accuracy = 97.32% | LR = 1.2\n",
            "Epoch 450: Accuracy = 97.33% | LR = 1.2\n",
            "Epoch 460: Accuracy = 97.36% | LR = 1.2\n",
            "Epoch 470: Accuracy = 97.34% | LR = 1.2\n",
            "Epoch 480: Accuracy = 97.35% | LR = 1.2\n",
            "Epoch 490: Accuracy = 97.39% | LR = 1.2\n",
            "Epoch 500: Accuracy = 97.42% | LR = 1.2\n",
            "Epoch 510: Accuracy = 97.42% | LR = 1.2\n",
            "Epoch 520: Accuracy = 97.45% | LR = 1.2\n",
            "Epoch 530: Accuracy = 97.46% | LR = 1.2\n",
            "Epoch 540: Accuracy = 97.47% | LR = 1.2\n",
            "Epoch 550: Accuracy = 97.48% | LR = 1.2\n",
            "Epoch 560: Accuracy = 97.49% | LR = 1.2\n",
            "Epoch 570: Accuracy = 97.49% | LR = 1.2\n",
            "Epoch 580: Accuracy = 97.50% | LR = 1.2\n",
            "Epoch 590: Accuracy = 97.51% | LR = 1.2\n",
            "Epoch 600: Accuracy = 97.54% | LR = 1.2\n",
            "Epoch 610: Accuracy = 97.55% | LR = 1.2\n",
            "Epoch 620: Accuracy = 97.57% | LR = 1.2\n",
            "Epoch 630: Accuracy = 97.59% | LR = 1.2\n",
            "Epoch 640: Accuracy = 97.59% | LR = 1.2\n",
            "Epoch 650: Accuracy = 97.59% | LR = 1.2\n",
            "Epoch 660: Accuracy = 97.60% | LR = 1.2\n",
            "Epoch 670: Accuracy = 97.60% | LR = 1.2\n",
            "Epoch 680: Accuracy = 97.64% | LR = 1.2\n",
            "Epoch 690: Accuracy = 97.64% | LR = 1.2\n",
            "Epoch 700: Accuracy = 97.63% | LR = 1.2\n",
            "Epoch 710: Accuracy = 97.63% | LR = 1.2\n",
            "Epoch 720: Accuracy = 97.63% | LR = 1.2\n",
            "Epoch 730: Accuracy = 97.64% | LR = 1.2\n",
            "Epoch 740: Accuracy = 97.66% | LR = 1.2\n",
            "Epoch 750: Accuracy = 97.68% | LR = 1.2\n",
            "Epoch 760: Accuracy = 97.68% | LR = 1.2\n",
            "Epoch 770: Accuracy = 97.70% | LR = 1.2\n",
            "Epoch 780: Accuracy = 97.71% | LR = 1.2\n",
            "Epoch 790: Accuracy = 97.73% | LR = 1.2\n",
            "Epoch 800: Accuracy = 97.74% | LR = 1.2\n",
            "Epoch 810: Accuracy = 97.77% | LR = 1.2\n",
            "Epoch 820: Accuracy = 97.77% | LR = 1.2\n",
            "Epoch 830: Accuracy = 97.78% | LR = 1.2\n",
            "Epoch 840: Accuracy = 97.78% | LR = 1.2\n",
            "Epoch 850: Accuracy = 97.79% | LR = 1.2\n",
            "Epoch 860: Accuracy = 97.80% | LR = 1.2\n",
            "Epoch 870: Accuracy = 97.79% | LR = 1.2\n",
            "Epoch 880: Accuracy = 97.77% | LR = 1.2\n",
            "Epoch 890: Accuracy = 97.78% | LR = 1.2\n",
            "Epoch 900: Accuracy = 97.78% | LR = 1.2\n",
            "Epoch 910: Accuracy = 97.78% | LR = 1.2\n",
            "Epoch 920: Accuracy = 97.80% | LR = 1.2\n",
            "Epoch 930: Accuracy = 97.80% | LR = 1.2\n",
            "Epoch 940: Accuracy = 97.79% | LR = 1.2\n",
            "Epoch 950: Accuracy = 97.80% | LR = 1.2\n",
            "Epoch 960: Accuracy = 97.80% | LR = 1.2\n",
            "Epoch 970: Accuracy = 97.84% | LR = 1.2\n",
            "Epoch 980: Accuracy = 97.84% | LR = 1.2\n",
            "Epoch 990: Accuracy = 97.85% | LR = 1.2\n",
            "Training completed in 229.58619141578674 seconds.\n",
            "Calculating final accuracy and graphing accuracy and cost histories...\n",
            "First few predictions:\t [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7]\n",
            "Expected labels:\t [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7]\n",
            "Accuracy: 97.84%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAI6CAYAAAAT/9SPAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdZ1JREFUeJzt3Xd8FHX+x/H3phdIQktCJDQbIvVAMAKiZ46IyIkVEQUR5eRAgdjAApx3CsLpYUFQT0VPFGygoqIYBEQjJRClCwqCQEJNQk39/v6Y326yySYkELKZ5PV8POYxuzPfnf3M7lfZd75THMYYIwAAAAAAbMrH2wUAAAAAAHAmCLYAAAAAAFsj2AIAAAAAbI1gCwAAAACwNT9vFwAAAACg+snPz1dubq63ywAkSQEBAfLxKX1clmALAAAAwMUYo7S0NGVkZHi7FMDFx8dHLVq0UEBAgMf1Dm73AwAAAMBp7969ysjIUGRkpEJCQuRwOLxdEmq5goIC7dmzR/7+/mratKnHPsmILQAAAABJ1uHHzlDboEEDb5cDuDRq1Eh79uxRXl6e/P39S6zn4lEAAAAAJMl1Tm1ISIiXKwHcOQ9Bzs/P97ieYAsAAADADYcfo7o5VZ8k2AIAAAAAbI1gCwAAAAC11MSJE9WhQwdvl3HGCLYAAAAAaozk5GT5+vqqT58+3i7Fq6644gqNHj26xPJZs2YpIiLC9fzBBx9UUlJSubZZnUMwwRYAAABAjfH666/rvvvu07Jly7Rnzx6v1pKTk+PV9y+POnXqVPkVsM/G50KwBYAa7M4771SdOnW8XQZqiVmzZsnhcGjHjh3eLgVALXX06FHNnTtXw4cPV58+fTRr1qwSbT777DNdcsklCgoKUsOGDXX99de71mVnZ+uRRx5RbGysAgMDdd555+n111+XVHKkU5Lmz5/vdlEj54jmf//7X7Vo0UJBQUGSpIULF6p79+6KiIhQgwYNdO211+rXX39129Yff/yhAQMGqH79+goNDVXnzp21YsUK7dixQz4+Plq9erVb+2nTpqlZs2YqKCg4k4+sxCjskiVL1KVLF4WGhioiIkLdunXT77//rlmzZukf//iHfvrpJzkcDjkcDtfnu3PnTl133XWqU6eOwsLCdMsttyg9Pb3Mz+Xtt99WgwYNlJ2d7VZPv379dMcdd1R4Pwi2AGoV5w/voKAg7d69u8T6K664Qm3atDmtbb/77ruaNm3aGVZYPT388MNyOBzq37+/t0upMU6ePKn//Oc/6tq1q8LDwxUUFKQLLrhAI0eO1C+//HJW3rOifbR58+a69tprPa5bsmSJHA6HPvzwwzOq6fjx45o4caKWLFlyRtsBcPYYIx075p3JmIrV+v7776tVq1a68MILdfvtt+uNN96QKbKRzz//XNdff72uueYarV27VklJSerSpYtr/aBBg/Tee+/phRde0KZNm/TKK69U+A/E27Zt00cffaSPP/5YqampkqRjx44pMTFRq1evVlJSknx8fHT99de7QunRo0fVs2dP7d69W59++ql++uknPfzwwyooKFDz5s0VHx+vN9980+193nzzTd15553y8am8SJeXl6d+/fqpZ8+e+vnnn5WcnKxhw4a5fgM88MADuvjii7V3717t3btX/fv3V0FBga677jodOnRIS5cu1aJFi/Tbb7+V+M1Q/HO5+eablZ+fr08//dTVZt++ffr888911113Vbx4AwC1yJtvvmkkGUlm5MiRJdb37NnTXHzxxae17T59+phmzZqdYYWVa/DgwSY0NPSMtlFQUGCaNGlimjdvboKDg01WVlYlVVd77d+/33Tq1MlIMtdee62ZNm2a+e9//2seeughExsba/z9/c/K+1a0jzZr1sz06dPH47pvv/3WSDIffPCBa1leXp45ceKEKSgoKPd77N+/30gyEyZMKPdrAJw9J06cMBs3bjQnTpxwLTt61BgrYlb9dPRoxeq/7LLLzLRp04wxxuTm5pqGDRuab7/91rU+Li7ODBw40ONrt2zZYiSZRYsWeVz/5ptvmvDwcLdl8+bNM0Uj1YQJE4y/v7/Zt29fmXU6/9+3bt06Y4wxr7zyiqlbt645ePCgx/Zz58419erVMydPnjTGGJOSkmIcDofZvn17qe/Rs2dP4+/vb0JDQ92mwMBAt/2YMGGCad++vTHGmIMHDxpJZsmSJR63WbSt09dff218fX3Nzp07Xcs2bNhgJJmVK1e6Xufpcxk+fLjp3bu36/mzzz5rWrZs6fHfEU99syhGbAHUSh06dNBrr73m9XNvzpZjx45V2raWLFmiP/74Q2+88Yby8vL08ccfV9q2K9vx48e9XUK53HnnnVq7dq0+/PBDffbZZxo1apSGDh2qKVOmaOvWrbr//vu9XeJp8fX1VVBQULW4/2Vl/jcAwB62bNmilStXasCAAZIkPz8/9e/f33UosSSlpqbqqquu8vj61NRU+fr6qmfPnmdUR7NmzdSoUSO3ZVu3btWAAQPUsmVLhYWFqXnz5pKsQ3id792xY0fVr1/f4zb79esnX19fzZs3T5J1BNqVV17p2k5pBg4cqNTUVLfpySefLLV9/fr1deeddyohIUF9+/bV888/r71795b5Hps2bVJsbKxiY2Ndy1q3bq2IiAht2rTJtczT53LPPffo66+/dh1FN2vWLN15552n9e8IwRZArfToo48qPz9fkydPLlf7d955R506dVJwcLDq16+vW2+9Vbt27XKtv+KKK/T555/r999/d5130rx5cxlj1LBhQyUmJrraFhQUKCIiQr6+vsrIyHAtf+aZZ+Tn56ejR4+6li1evFg9evRwnedy3XXXuf0jIVnnrTgcDm3cuFG33Xab6tWrp+7du5e6L6mpqWrUqJGuuOIKt/cqzezZs9W6dWtdeeWVio+P1+zZsz222717t4YOHaqYmBgFBgaqRYsWGj58uNsFIjIyMjRmzBg1b95cgYGBatKkiQYNGqQDBw5IKv0cTedhr0UPV3UeNp6SkqLLL79cISEhevTRRyVJn3zyifr06eOq5dxzz9U///lP5efnl6h7xYoVuuaaa1SvXj2FhoaqXbt2ev755yVZh3k5HA6tXbu2xOuefvpp+fr6ejykvSwrVqzQ559/rqFDh+rGG28ssT4wMFD//ve/3ZaVpx8cOXJEo0ePdn22kZGR+stf/qI1a9a4Pi9PfbQyefr+Vq9erYSEBDVs2FDBwcFq0aKF6xCzHTt2uH7k/OMf/3DVNXHixArte2n/DZyN7w+ojUJCpKNHvTOFhJS/ztdff115eXmKiYmRn5+f/Pz8NGPGDH300UfKzMyUJAUHB5f6+rLWSZKPj4/bYc2SlJubW6JdaGhoiWV9+/bVoUOH9Nprr2nFihVasWKFpMKLKJ3qvQMCAjRo0CC9+eabysnJ0bvvvluuw3XDw8N13nnnuU2RkZFlvubNN99UcnKyLrvsMs2dO1cXXHCBfvzxx1O+16l4+lw6duyo9u3b6+2331ZKSoo2bNigO++887S273eG9QGALbVo0UKDBg3Sa6+9prFjxyomJqbUtk899ZSeeOIJ3XLLLbr77ru1f/9+vfjii7r88su1du1aRURE6LHHHlNmZqb++OMP/ec//5FkXWXQ4XCoW7duWrZsmWt7P//8szIzM+Xj46Pvv//edTuC7777Th07dnSdy/PNN9+od+/eatmypSZOnKgTJ07oxRdfVLdu3bRmzZoSoeTmm2/W+eefr6effrrEP7xOq1atUkJCgjp37qxPPvnklP+QZmdn66OPPtIDDzwgSRowYICGDBmitLQ0RUdHu9rt2bNHXbp0UUZGhoYNG6ZWrVpp9+7d+vDDD3X8+HEFBATo6NGj6tGjhzZt2qS77rpLf/rTn3TgwAF9+umn+uOPP9SwYcMya/Hk4MGD6t27t2699VbdfvvtioqKkmQFrDp16igxMVF16tTR4sWLNX78eGVlZWnq1Kmu1y9atEjXXnutGjdurFGjRik6OlqbNm3SggULNGrUKN10000aMWKEZs+erY4dO7q99+zZs3XFFVfonHPOqVDNznOJynthjPL2g3vvvVcffvihRo4cqdatW+vgwYNavny5Nm3apD/96U+l9tFTyc3Ndf3hoSjnj8Sy7Nu3T7169VKjRo00duxYRUREaMeOHa5R/0aNGmnGjBkaPny4rr/+et1www2SpHbt2lVo352K/zdwNr4/oDZyOCQPmaRaycvL09tvv61nn31WvXr1clvXr18/vffee7r33nvVrl07JSUlaciQISW20bZtWxUUFGjp0qWKj48vsb5Ro0Y6cuSIjh075gppznNoy3Lw4EFt2bJFr732mnr06CFJWr58uVubdu3a6b///a8OHTpU6qjt3XffrTZt2ujll19WXl6e6/+ZZ0PHjh3VsWNHjRs3TnFxcXr33Xd16aWXKiAgoMQfiS+66CLt2rVLu3btco3abty4URkZGWrduvUp3+vuu+/WtGnTtHv3bsXHx7uN/FaIxwOUAaCGcp5ju2rVKvPrr78aPz8/c//997vWFz/HdseOHcbX19c89dRTbttZt26d8fPzc1te2vmLU6dONb6+vq5zU1944QXTrFkz06VLF/PII48YY4zJz883ERERZsyYMa7XdejQwURGRrqdb/PTTz8ZHx8fM2jQINeyCRMmGElmwIABJd676Dm2y5cvN2FhYaZPnz6uc3RO5cMPPzSSzNatW40xxmRlZZmgoCDzn//8x63doEGDjI+Pj1m1alWJbTjPkxk/fryRZD7++ONS2zi/n+LnDDnP5yx6nlTPnj2NJDNz5swS2zt+/HiJZX/7299MSEiIa9/z8vJMixYtTLNmzczhw4c91mOMMQMGDDAxMTEmPz/ftWzNmjVGknnzzTdLvM+pXH/99UZSifcsTXn7QXh4uBkxYkSZ2zqdc2z1/+eklzYVPce2+PfnPPfMU79wKusc28r4b6Cyvz+gpjvVeYzV1bx580xAQIDJyMgose7hhx82nTt3NsZY/574+PiY8ePHm40bN5qff/7ZTJ482dX2zjvvNLGxsWbevHnmt99+M99++62ZO3euMcY6/zQ0NNTcf//9Ztu2bWb27NkmJiamxDm2xc9Bzc/PNw0aNDC333672bp1q0lKSjKXXHKJkWTmzZtnjDEmOzvbXHDBBaZHjx5m+fLl5tdffzUffvih+eGHH9y2ddlll5mAgABz7733nvIz6dmzpxk1alSJ5cXPFS5a82+//WbGjh1rfvjhB7Njxw7z1VdfmQYNGpiXX37ZGGPM7NmzTWhoqFm7dq3Zv3+/OXnypCkoKDAdOnQwPXr0MCkpKWbFihWmU6dOpmfPnmV+Lk4ZGRkmJCTEBAQEmDlz5pS6P5xjCwClaNmype644w69+uqrpZ4/8vHHH6ugoEC33HKLDhw44Jqio6N1/vnn69tvvz3l+/To0UP5+fn64YcfJFkjsz169FCPHj303XffSZLWr1+vjIwM119y9+7dq9TUVN15551uf7lt166d/vKXv+iLL74o8T733ntvqTV8++23SkhI0FVXXaWPP/5YgYGBp6xbska1OnfurPPOO0+SVLduXfXp08ftcOSCggLNnz9fffv2VefOnUtsw3mezEcffaT27du73VaheJuKCgwM9PhX96Ij0UeOHNGBAwfUo0cPHT9+XJs3b5YkrV27Vtu3b9fo0aNL3L6haD2DBg3Snj173L7r2bNnKzg42OOhxKeSlZUlyfosT6Ui/SAiIkIrVqyo9PPGu3btqkWLFpWYih8u7Ynzc12wYIHHw/XKUln/DVT29wegenr99dcVHx+v8PDwEutuvPFGrV69Wj///LOuuOIKffDBB/r000/VoUMH/fnPf9bKlStdbWfMmKGbbrpJf//739WqVSvdc889rnP269evr3feeUdffPGF2rZtq/fee8/t1InS+Pj4aM6cOUpJSVGbNm00ZswYt6OHJOtQ46+//lqRkZG65ppr1LZtW02ePFm+vr5u7YYOHaqcnJzTu2pwOYSEhGjz5s268cYbdcEFF2jYsGEaMWKE/va3v0myPsurr75aV155pRo1aqT33ntPDodDn3zyierVq6fLL79c8fHxatmypebOnVuu9wwPD9eNN96oOnXqqF+/fqdffJkxHwBqmKIjtsaYEqO2xUdshw8fXuZoVbt27VxtSxsNy8nJMSEhIeaxxx4zxhgTHR1tXnnlFfPJJ5+YgIAAc+LECfPSSy8ZSa6rBSYnJxtJ5vXXXy+xvdGjRxtJ5uj/XyrSOVpV9GqEToMHDza+vr4mODjYdO/e3eTm5pb7szp8+LAJDAw0DzzwgNm6datreu6554wks2XLFmOMMWlpaUaSa/9KExQUVOqVKJ0qOmLbsmVLj9tZv3696devnwkLCyvxnS1dutQYY8ycOXPKvPqlU15enmncuLEZMmSIMcb6y3tMTIy59dZby3xdaSoyYluRfjB37lwTFBRkfHx8zCWXXGImTJhgfv31V7fXnO2rIhf//goKCsyNN95oJJmwsDDz17/+1bzxxhtuRwyUNmJbWf8NVPb3B9R0dh2xrS2efPJJ07ZtW2+XUen+/Oc/m/vuu6/MNozYAkAZWrZsqdtvv73UUduCggI5HA4tXLjQ46jVK6+8csr38Pf3V9euXbVs2TJt27ZNaWlp6tGjh7p3767c3FytWLFC3333nVq1alXiaoEVUdr5soGBgerTp49WrFihhQsXlnt7H3zwgbKzs/Xss8/q/PPPd03OC2GVdhGpM1HayK2niz5Jnvc5IyNDPXv21E8//aQnn3xSn332mRYtWqRnnnlGkip8I3tfX1/ddttt+uijj3Ty5El9++232rNnj26//fYKbcepVatWkqR169ad1utLc8stt+i3337Tiy++qJiYGE2dOlUXX3yxvvzyy0p9n4pw3uc2OTlZI0eO1O7du3XXXXepU6dO5bpwWUV56g+V/f0BgDccPXpU69ev10svvaT77rvP2+VUmsOHD2vevHlasmSJRowYcUbbItgCqPUef/xx5eXluYJPUeeee66MMWrRooXi4+NLTJdeeqmrbVmH0/bo0UMrV67UN998o4YNG6pVq1aqX7++Lr74Yn333Xf67rvvdPnll7vaN2vWTJJ164LiNm/erIYNG3q8uqAnDodDs2fP1lVXXaWbb77Z7crCZZk9e7batGmjDz74oMQUHx+vd999V5J1MY2wsDCtX7++zO2de+65p2xTr149SXK7WrQk/f777+WqWbKuoHzw4EHNmjVLo0aN0rXXXqv4+HjXtovWI+mUNUnW4axZWVn67LPPNHv2bDVq1EgJCQnlrqmovn37SrKutH0qFe0HjRs31t///nfNnz9f27dvV4MGDfTUU0+51nvrNjyXXnqpnnrqKa1evVqzZ8/Whg0bNGfOnDJrqsz/Birz+wMAbxg5cqQ6deqkK6644qwdhuwNHTt21J133qlnnnlGF1544Rlti2ALoNY799xzdfvtt+uVV15RWlqa27obbrhBvr6++sc//lHiSsPGGB08eND1PDQ0tNQrxfbo0UPZ2dmaNm2aunfv7vox36NHD/3vf//Tnj17XOfXSlZA6dChg9566y23kLd+/Xp9/fXXuuaaayq0jwEBAfr44491ySWXqG/fvm7nE3mya9cuLVu2TLfccotuuummEtOQIUO0bds2rVixQj4+PurXr58+++wzrV69usS2nJ/bjTfeqJ9++sl1Dz5PbZxhs+hVpPPz8/Xqq6+We1+d5yMV/b5ycnL08ssvu7X705/+pBYtWmjatGklgnTx77pdu3auK1Z+9NFHuvXWW+Xnd3o3FoiLi9PVV1+t//73v5o/f36J9Tk5OXrwwQcllb8f5Ofnl+h7kZGRiomJUXZ2tmtZWX30bDh8+HCJz7JDhw6S5Kor5P/v5VH8O6jM/wYq8/sDAG+YNWuWsrOzNXfu3BLn3drZjh07lJmZ6fp370zwf3UAkPTYY4/pf//7n7Zs2aKLL77Ytfzcc8/Vv/71L40bN047duxQv379VLduXW3fvl3z5s3TsGHDXP8z7tSpk+bOnavExERdcsklqlOnjmt0Li4uTn5+ftqyZYuGDRvm2v7ll1+uGTNmSJJbsJWkqVOnqnfv3oqLi9PQoUNdtzoJDw8v18UqigsODtaCBQv05z//Wb1799bSpUvVpk0bj23fffddGWP017/+1eP6a665Rn5+fpo9e7a6du2qp59+Wl9//bV69uypYcOG6aKLLtLevXv1wQcfaPny5YqIiNBDDz2kDz/8UDfffLPrcNRDhw7p008/1cyZM9W+fXtdfPHFuvTSSzVu3DjXLQ/mzJmjvLy8cu/nZZddpnr16mnw4MG6//775XA49L///a9EwPLx8dGMGTPUt29fdejQQUOGDFHjxo21efNmbdiwQV999ZVb+0GDBrm+a0+HsS5ZskRXXnmlJkyYcMrv5+2331avXr10ww03qG/fvrrqqqsUGhqqrVu3as6cOdq7d6/r4kzl6QdHjhxRkyZNdNNNN6l9+/aqU6eOvvnmG61atUrPPvus633L6qNnw1tvvaWXX35Z119/vc4991wdOXJEr732msLCwlzBNDg4WK1bt3bdK7F+/fpq06aN2rRpU6n/DZzq+wMA2FzlnfILANVf8YtHFTV48GAjye3iUU4fffSR6d69uwkNDTWhoaGmVatWZsSIEa4LKBljzNGjR81tt91mIiIijKQSF+lxXtp/xYoVrmV//PGHkWRiY2M91vvNN9+Ybt26meDgYBMWFmb69u1rNm7c6NbGeeGc/fv3e9wn5+1+nA4cOGBat25toqOjXbfxKa5t27amadOmHtc5XXHFFSYyMtJ1Qarff//dDBo0yDRq1MgEBgaali1bmhEjRpjs7GzXaw4ePGhGjhxpzjnnHBMQEGCaNGliBg8ebA4cOOBq8+uvv5r4+HgTGBhooqKizKOPPmoWLVrk8eJRnr4rY4z5/vvvzaWXXmqCg4NNTEyMefjhh81XX31VYhvGWLdB+stf/mLq1q1rQkNDTbt27cyLL75YYpt79+41vr6+5oILLvD4np999lmptx/y5Pjx4+bf//63ueSSS0ydOnVMQECAOf/88819991ntm3b5tb2VP0gOzvbPPTQQ6Z9+/au/Wjfvr3r9gxOp+qjxZ3pxaPWrFljBgwYYJo2bWoCAwNNZGSkufbaa83q1avdtvXDDz+YTp06mYCAgBIXkjrT/wacTvX9AbBw8ShUV6fqmw5jiv0JGwAAlHDgwAE1btxY48eP1xNPPFFi/cMPP6z33ntP27ZtK/ftlFB1TvX9AbCcPHlS27dvV4sWLRQUFOTtcgCXU/VNzrEFAKAcZs2apfz8fN1xxx0e13/77bd64oknCLXV1Km+PwCAvXGOLQAAZVi8eLE2btyop556Sv369VPz5s09tlu1alXVFoZyKe/3BwCwN4ItAABlePLJJ/XDDz+oW7duevHFF71dDiqI7w8AageCLQAAZSjvfX9RPfH9AajOduzYoRYtWmjt2rWu26Hh9HCOLQAAAIAaIS0tTffdd59atmypwMBAxcbGqm/fvkpKSqqU7c+aNUsRERFn1M7hcLjuYx4bG6u9e/eWevu9onbs2CGHw6HU1NTyF1yLMGILAAAAwPZ27Nihbt26KSIiQlOnTlXbtm2Vm5urr776SiNGjNDmzZu9XWIJvr6+io6OrvL3zc3Nlb+/f5W/79nEiC0AAAAA2/v73/8uh8OhlStX6sYbb9QFF1ygiy++WImJifrxxx9d7Xbu3KnrrrtOderUUVhYmG655Ralp6e71v/000+68sorVbduXYWFhalTp05avXq1lixZoiFDhigzM1MOh0MOh0MTJ048o5qLj8IePnxYAwcOVKNGjRQcHKzzzz9fb775piSpRYsWkqSOHTvK4XDoiiuukCQVFBToySefVJMmTRQYGKgOHTpo4cKFJd5j7ty56tmzp4KCgvTqq68qLCxMH374oVs98+fPV2hoqI4cOXJG++UNjNgCAAAAKJ0x0vHj3nnvkBDJ4Thls0OHDmnhwoV66qmnFBoaWmK987DggoICV6hdunSp8vLyNGLECPXv3991Tv7AgQPVsWNHzZgxQ76+vkpNTZW/v78uu+wyTZs2TePHj9eWLVskSXXq1Km0XZWkJ554Qhs3btSXX36phg0batu2bTpx4oQkaeXKlerSpYu++eYbXXzxxQoICJAkPf/883r22Wf1yiuvqGPHjnrjjTf017/+VRs2bND555/v2vbYsWP17LPPqmPHjgoKCtJPP/2kN998UzfddJOrjfN53bp1K3W/qgLBFgAAAEDpjh+XKjnAldvRo5KHoFrctm3bZIxRq1atymyXlJSkdevWafv27YqNjZUkvf3227r44ou1atUqXXLJJdq5c6ceeugh17aKhsPw8HA5HI5yHT6cmZlZ4eC7c+dOdezYUZ07d5Ykt1uUNWrUSJLUoEEDt/f/97//rUceeUS33nqrJOmZZ57Rt99+q2nTpmn69OmudqNHj9YNN9zgen733Xfrsssu0969e9W4cWPt27dPX3zxhb755psK1VxdcCgyAAAAAFszxpSr3aZNmxQbG+sKtZLUunVrRUREaNOmTZKkxMRE3X333YqPj9fkyZP166+/nlZNdevWVWpqaompLMOHD9ecOXPUoUMHPfzww/rhhx/KbJ+VlaU9e/aoW7dubsu7devm2h8nZ1h26tKliy6++GK99dZbkqR33nlHzZo10+WXX17OPaxeCLYAAAAAShcSYo2cemMKCSlXieeff74cDkelXCBq4sSJ2rBhg/r06aPFixerdevWmjdvXoW34+Pjo/POO6/EVJbevXvr999/15gxY7Rnzx5dddVVevDBB093V9x4OkT77rvv1qxZsyRZhyEPGTJEjnIc+l0dEWwBAAAAlM7hsA4H9sZUzpBVv359JSQkaPr06Tp27FiJ9RkZGZKkiy66SLt27dKuXbtc6zZu3KiMjAy1bt3ateyCCy7QmDFj9PXXX+uGG25wXcApICBA+fn5Z/BhnlqjRo00ePBgvfPOO5o2bZpeffVV13tLcnv/sLAwxcTE6Pvvv3fbxvfff++2P6W5/fbb9fvvv+uFF17Qxo0bNXjw4Erck6pFsAUAAABge9OnT1d+fr66dOmijz76SFu3btWmTZv0wgsvKC4uTpIUHx+vtm3bauDAgVqzZo1WrlypQYMGqWfPnurcubNOnDihkSNHasmSJfr999/1/fffa9WqVbroooskWee8Hj16VElJSTpw4ICOV/JFtcaPH69PPvlE27Zt04YNG7RgwQLXe0dGRio4OFgLFy5Uenq6MjMzJUkPPfSQnnnmGc2dO1dbtmzR2LFjlZqaqlGjRp3y/erVq6cbbrhBDz30kHr16qUmTZpU6v5UJYItAAAAANtr2bKl1qxZoyuvvFIPPPCA2rRpo7/85S9KSkrSjBkzJEkOh0OffPKJ6tWrp8svv1zx8fFq2bKl5s6dK8m6r+zBgwc1aNAgXXDBBbrlllvUu3dv/eMf/5AkXXbZZbr33nvVv39/NWrUSFOmTKnUfQgICNC4cePUrl07XX755fL19dWcOXMkSX5+fnrhhRf0yiuvKCYmRtddd50k6f7771diYqIeeOABtW3bVgsXLtSnn37qdtGrsgwdOlQ5OTm66667KnVfqprDlPdMawAAAAA12smTJ7V9+3a1aNFCQUFB3i4HVeB///uf65xe5+HO1dGp+ia3+wEAAACAWub48ePau3evJk+erL/97W/VOtSWB4ciAwAAAEAtM2XKFLVq1UrR0dEaN26ct8s5YxyKDAAAAEAShyKj+jpV32TEFgAAAABgawRbAAAAAG44qBPVzan6JMEWAAAAgCTJ399fkir9/qzAmcrJyZFk3ZLJE66KDAAAAECSFRoiIiK0b98+SVJISIgcDoeXq0JtV1BQoP379yskJER+fp4jLMEWAAAAgEt0dLQkucItUB34+PioadOmpf6hhasiAwAAACghPz9fubm53i4DkCQFBATIx6f0M2kJtgAAAAAAW+PiUQAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABszc/bBVRnBQUF2rNnj+rWrSuHw+HtcgAAAAB4iTFGR44cUUxMjHx8GB+sbgi2ZdizZ49iY2O9XQYAAACAamLXrl1q0qSJt8tAMbYJtsuWLdPUqVOVkpKivXv3at68eerXr1+Zr1myZIkSExO1YcMGxcbG6vHHH9edd95Z7vesW7euJKvzhoWFnUH1AAAAAOwsKytLsbGxroyA6sU2wfbYsWNq37697rrrLt1www2nbL99+3b16dNH9957r2bPnq2kpCTdfffdaty4sRISEsr1ns7Dj8PCwgi2AAAAADhFsZqyTbDt3bu3evfuXe72M2fOVIsWLfTss89Kki666CItX75c//nPf0oNttnZ2crOznY9z8rKOrOiAQAAAABnXY096zk5OVnx8fFuyxISEpScnFzqayZNmqTw8HDXxPm1AAAAAFD91dhgm5aWpqioKLdlUVFRysrK0okTJzy+Zty4ccrMzHRNu3btqopSAQAAAABnwDaHIleFwMBABQYGersMAAAAAEAF1NgR2+joaKWnp7stS09PV1hYmIKDg71UFQAAAACgstXYEdu4uDh98cUXbssWLVqkuLg4L1UEAAAAT4yxJklyOKxJkgoKrLnP/w/F5OVJ2dnWc+dkjNXOOXna9qmWladNVSyzYx15eVJOjvXY19f6DpzPAwKk3FzreUBA4fqCgsLtOb/7osuLfr/OvnDRRVJQUMkaACfbBNujR49q27Ztrufbt29Xamqq6tevr6ZNm2rcuHHavXu33n77bUnSvffeq5deekkPP/yw7rrrLi1evFjvv/++Pv/8c2/tAgCgmnH+mMrPt36cFZ1ycqwfZA5H4Q+sggJrXX6+NRX98VV0O84fbM4fc87HznXFf9CVtuxUz0/3NZ5+SJY1L08b52finJzhxPnD1Pnj9FTbKe+8tB/BxWsu+jk437/ovDovc/ZDX1/p+HHJ31/y8yv5eRfto8a49zVnm1N9DqeaV/Q1R4+611p0GydOuAfQ/PzCZb6+1nN//8LPQLKWOxyFz1H7bNxohVugNLYJtqtXr9aVV17pep6YmChJGjx4sGbNmqW9e/dq586drvUtWrTQ559/rjFjxuj5559XkyZN9N///rfc97AFUDs5w4pU8gd0RabcXOuHWvHJ+aPMU8g41fITJ6wft0V5Ch7Ff/Q7H+fnF/7lvGib/HxrWdHJGdDy80uGhOKB4VTPy2pTvNbSptI+mzNpW3TEAED14AzBubmel9d2nm6fWp5lldXG0zJ/f2ty/uHB17fwD3q5udbzoKDCf1t8fd1HYiX3P05J7n8wc/KzTWqBtziM4Z/10mRlZSk8PFyZmZkKCwvzdjlAteAMBc7g4wxKzhBUdJ6bKx07Zk3OQ8ecIwvOw8k8BSrJPYR5mjv/wStrnptrvUfRKSfHqiEzs3B/cnIK6yz+Ywq1V0CA9UOq6A8sX19r8vMr/AHmXOfjYy13/vgqKCj8MWeMFBhYOOpUdASz6HNPy071/HReU/S/RYejsK7iy4seCniqufOzcU7Oz8DT4aFlvd+p6nC+V2k/gj291qnoZ1F0Xl2XOQNDbq4UGlr4/1UfH8/9sKDAem3RvuZs62xX1ntX5PM51TwkxKrX+f7O0VgfHyk4uLCPSIXLnP82BAYW/ptRNCAVFFjtAgLc/y0q/p1XVmCrDiES1QvZoHrjbx9ANWRMYYhzhrGiIbB4iCz6+MQJK7QdPSqdPGk9P3my5FR8eW5u4ft6ek9n4DtyxLufTXVW9Me2n5/1Ayw42PqB53zs51d6ODnV8oAAqW7dkj9+iv6ALR5Eii8LCLB+KBet1dfX+iEZEOC+3s/PvV3xH46enpenTWnh5XSm0j6vU03FPxfnIZ7OfXZ+pgAAwB4ItsBpKCiwDgl1Hl5aNCgWnZ84IR0+LP3xhzUamJ8vZWRIhw5Z8+KHqRYNlHZUPBD5+Ul16lgjDYGB7oee+vlZIapooHJeWEIq/Eu9v3/JedHw4Qxenh77+VnbLzr5+1t/BGjQwD0whoZaU3Bw4ahx8WBY2uRphAAAAABVh2CLWskZIHftkrZutULqoUPSgQPWyGRenpSVZY18ZmZaIbTo46oetfTxcQ9mRYNj0RDnHHmLiLACpXOUMCio9Mm53jmS6AyYzvdzPvbzcw+EzvctOsIHAAAAeAPBFrZljLR7t7RjR+H5kfv3W/Pdu62gum2b9Ntv1gjcsWPWa7KzC68QWRkCAwvDoad5SIgUGWkFzbAwK3Q2aCCFh5c8TLVokCz6mNAIAAAAlI5gi2ojPV369VcriP7xhzVimp5ujapmZ1ujpCdPWvP8fGv9sWNn9p5hYVKzZtZ5i9HRVuB0jl7WrWuFz4gI97nzcWio1bbohUkAAAAAVD2CLapEbq60c6e0aZMVXPfvt0Zajx61RlZTUk4vpPr6FgbTkBCpUSNr5POcc6wA6uNjjZSGhxcGV+dIaViY9ZhzIwEAAAB7I9jirNm7V3r9dWn+fOum2idOnPo1wcFS06ZS48bW1KyZFUjr1JGioqxR0rp1rRHVkBCpefPCWwEAAAAAqJ0Itqh0ublSYqL08ssl713YsqUUGyvFxEgdO1ojpnXrShdcYAXZmBjv1AwAAADAvgi2KDfnzdEdDuuKwsVvsO5sc+210tdfW8/j4qQBA6TOna3J37/q6wYAAABQsxFs4VFenvTOO9KPPxZezOn3363b4hQVFye9+qrUpo31/M03rVAbGiq9+670179Wfe0AAAAAaheCLUp45x3pscesiz2dSnKyNRLbv7907rnS1KnW8qeeItQCAAAAqBoEW7gUFEiDB1vBVrKuGnznnVK7doXnv0ZFWbfb2b9fWrZMeu89ad066e23C7dz5ZXSvfd6ZRcAAAAA1EIOY4zxdhHVVVZWlsLDw5WZmamwsDBvl3PWff+91L279TgxUZowwQq3ZTHGOvx48mRp61apdWtp5UrrUGQAAACgpqht2cBufLxdAKqPefOs+S23SM8+e+pQK1kXkrrrLumXX6x71BJqAQAAAFQ1gm0tdfiwNdpa1BdfWPObbjq9bbZqRagFAAAAUPUItrXMpEnWKGv9+tYFopz27bNGXCXpz3/2Tm0AAAAAcDoItrXI9u3uYXbSJGuZJH33nTVv00Zq0KDqawMAAACA00WwrUVmzSp5+HH37tLRo9Lq1dbzbt2qvCwAAAAAOCME21pk82Zr/txzhUF2zx7ps8+siz9J0kUXeac2AAAAADhdBNsaLDdXOniw8Plvv1nzli2lTp0KD0ueO9e6VY8knX9+1dYIAAAAAGeKYFuD3XOPFBMjrVljPS8abCXpxhut+ddfSxs2WI8JtgAAAADshmBbQ508Kb31lpSTIz36qJSRIR06ZK1r0cKad+ggRUdLJ05IBQWSn5/UvLmXCgYAAACA00SwraGcVzmWpJSUwtHaRo2kOnWsxw6HdN11he0uvVTy96+6GgEAAACgMvh5uwCcHc6LQ0nSgQPStm3W4+ho93aTJ1vzffukl16qmtoAAAAAoDIRbGuokyfdn+/YYc1DQ92XR0RIM2dWRUUAAAAAcHZwKHINlZvr/nz7dmtePNgCAAAAgN0RbGuonBz3584R25CQKi8FAAAAAM4qgm0NxYgtAAAAgNqCYFtDMWILAAAAoLYg2NZQzhFbn///hrOzrTkjtgAAAABqGoJtDeUcsT3nHPflBFsAAAAANQ3BtoZyjtg2buy+nEORAQAAANQ0BNsayjliW7+++3JGbAEAAADUNATbGso5Yls82DJiCwAAAKCmIdjWUIzYAgAAAKgtCLY1FCO2AAAAAGoLgm0NcviwtG+f9dg5YluvnnsbRmwBAAAA1DQE2xrCGKlLFykmRjp4sPQRW4ItAAAAgJrGz9sFoHIcOSJt22Y9fv/90kdsORQZAAAAQE1juxHb6dOnq3nz5goKClLXrl21cuXKMttPmzZNF154oYKDgxUbG6sxY8bo5MmTVVRt1cnIKHz85ZeFwTY0VAoIKFzHiC0AAACAmsZWwXbu3LlKTEzUhAkTtGbNGrVv314JCQna5zyxtJh3331XY8eO1YQJE7Rp0ya9/vrrmjt3rh599NEqrvzsKxpsf/+98FBkf38pMLBwXUREVVYFAAAAAGefrYLtc889p3vuuUdDhgxR69atNXPmTIWEhOiNN97w2P6HH35Qt27ddNttt6l58+bq1auXBgwYcMpRXjvKzCx8fORI4YhtQID13CkysmrrAgAAAICzzTbBNicnRykpKYqPj3ct8/HxUXx8vJKTkz2+5rLLLlNKSooryP7222/64osvdM0113hsn52draysLLfJLoqO2GZluY/YFuVjm28cAAAAAMrHNhePOnDggPLz8xUVFeW2PCoqSps3b/b4mttuu00HDhxQ9+7dZYxRXl6e7r333lIPRZ40aZL+8Y9/VHrtVaFosD1ypDDAFj2/FgAAAABqoho9frdkyRI9/fTTevnll7VmzRp9/PHH+vzzz/XPf/7TY/tx48YpMzPTNe3atauKKz59RYNtTo509Kj1uOiIbVBQlZYEAAAAAFXCNiO2DRs2lK+vr9LT092Wp6enKzo62uNrnnjiCd1xxx26++67JUlt27bVsWPHNGzYMD322GPyKXZcbmBgoAKLXmnJRooGW0k6ccKaFx2xbdiwysoBAAAAgCpjmxHbgIAAderUSUlJSa5lBQUFSkpKUlxcnMfXHD9+vER49fX1lSQZY85esV5Q9OJRRfn7S88/b43WzplTtTUBAAAAQFWwTbCVpMTERL322mt66623tGnTJg0fPlzHjh3TkCFDJEmDBg3SuHHjXO379u2rGTNmaM6cOdq+fbsWLVqkJ554Qn379nUFXDvbvVt6+mlp796SI7ZOAQHS/fdbF5Tq1q1KywMAAACAKmGbQ5ElqX///tq/f7/Gjx+vtLQ0dejQQQsXLnRdUGrnzp1uI7SPP/64HA6HHn/8ce3evVuNGjVS37599dRTT3lrFyrNyZNSkybW4+PHSw+2znNsi18dGQAAAABqCoepacfkVqKsrCyFh4crMzNTYWFh3i7HzfLlUo8e1uPbbpMOHJC+/rpku6NHpdDQqq0NAAAAqGmqczaAzQ5FRqGTJwsf+/tbV0L2hJFaAAAAADUdwdamigbZEyek7GzP7Qi2AAAAAGo6gq1NlTfYOhxVUw8AAAAAeAvB1qaKB1vn8yuv9E49AAAAAOAtBFubKhpsjx8vHLF97DHpwgu9UxMAAAAAeAPB1qZKOxQ5LEwaMMA7NQEAAACANxBsbaq0YBsQIPXvbz3+/9v7AgAAAECN5uftAnB6il4squg5toGBUqtW0saNUqNG3qkNAAAAAKoSwdamShuxDQy05hddVPU1AQAAAIA3cCiyTZ0q2AIAAABAbcGIrU0VDbZHj0rGWI8DArxTDwAAAAB4CyO2NlU02DpDrcSILQAAAIDah2BrU0WDbVEEWwAAAAC1DcHWpkoLtv7+VVsHAAAAAHgbwdamPAXbgADJ4aj6WgAAAADAmwi2NuUp2HIYMgAAAIDaiGBrUwRbAAAAALAQbG2KYAsAAAAAFoKtTZV2ji0AAAAA1DYEW5tixBYAAAAALARbm3IG2+DgwmUEWwAAAAC1EcHWppzBtnnzwmUEWwAAAAC1EcHWppzBtlmzwmWcYwsAAACgNiLY2pSnYMuILQAAAIDaiGBrUwRbAAAAALAQbG3KGWwvvLBwWV6ed2oBAAAAAG8i2NqUM9g2bSo9/7xUt6701796tyYAAAAA8AY/bxeA0+MMtgEB0v33SyNHSj78mQIAAABALUQUsqmiwVYi1AIAAACovYhDNpWdbc25xQ8AAACA2o5ga0PGSMePW49DQrxbCwAAAAB4G8HWhnJyrHArScHB3q0FAAAAALyNYGtDztFaiRFbAAAAACDY2tCJE9bc11fy9/duLQAAAADgbQRbG+L8WgAAAAAoRLC1IeeILefXAgAAAADB1pYYsQUAAACAQgRbG2LEFgAAAAAKEWxtiBFbAAAAAChEsLUhRmwBAAAAoJDtgu306dPVvHlzBQUFqWvXrlq5cmWZ7TMyMjRixAg1btxYgYGBuuCCC/TFF19UUbVnByO2AAAAAFDIz9sFVMTcuXOVmJiomTNnqmvXrpo2bZoSEhK0ZcsWRUZGlmifk5Ojv/zlL4qMjNSHH36oc845R7///rsiIiKqvvhK5Ay2jNgCAAAAgM2C7XPPPad77rlHQ4YMkSTNnDlTn3/+ud544w2NHTu2RPs33nhDhw4d0g8//CB/f39JUvPmzauy5LPCeSgyI7YAAAAAYKNDkXNycpSSkqL4+HjXMh8fH8XHxys5Odnjaz799FPFxcVpxIgRioqKUps2bfT0008rPz/fY/vs7GxlZWW5TdURI7YAAAAAUMg2wfbAgQPKz89XVFSU2/KoqCilpaV5fM1vv/2mDz/8UPn5+friiy/0xBNP6Nlnn9W//vUvj+0nTZqk8PBw1xQbG1vp+1EZGLEFAAAAgEK2Cbano6CgQJGRkXr11VfVqVMn9e/fX4899phmzpzpsf24ceOUmZnpmnbt2lXFFZcPI7YAAAAAUMg259g2bNhQvr6+Sk9Pd1uenp6u6Ohoj69p3Lix/P395evr61p20UUXKS0tTTk5OQoICHBrHxgYqMDAwMovvpIxYgsAAAAAhWwzYhsQEKBOnTopKSnJtaygoEBJSUmKi4vz+Jpu3bpp27ZtKigocC375Zdf1Lhx4xKh1k643Q8AAAAAFLJNsJWkxMREvfbaa3rrrbe0adMmDR8+XMeOHXNdJXnQoEEaN26cq/3w4cN16NAhjRo1Sr/88os+//xzPf300xoxYoS3dqFSnDxpzYOCvFsHAAAAAFQHtjkUWZL69++v/fv3a/z48UpLS1OHDh20cOFC1wWldu7cKR+fwqweGxurr776SmPGjFG7du10zjnnaNSoUXrkkUe8tQuVIjvbmtvgqGkAAAAAOOscxhjj7SKqq6ysLIWHhyszM1NhYWHeLsflmmukL7+U3nxTuvNOb1cDAAAA1HzVNRvAYqtDkWFhxBYAAAAAChFsbYhgCwAAAACFCLY2RLAFAAAAgEIEWxsi2AIAAABAIYKtDRFsAQAAAKAQwdaGCLYAAAAAUIhga0MEWwAAAAAoRLC1IYItAAAAABQi2NoQwRYAAAAAChFsbYhgCwAAAACFCLY2k59vTRLBFgAAAAAkgq3tOEdrJYItAAAAAEgEW9sh2AIAAACAO4KtzTiDrcMh+fl5txYAAAAAqA4ItjZT9MJRDod3awEAAACA6oBgazNcERkAAAAA3BFsbYZgCwAAAADuCLY2Q7AFAAAAAHcEW5sh2AIAAACAO4KtzRBsAQAAAMAdwdZmCLYAAAAA4I5gazMEWwAAAABwR7C1GYItAAAAALgj2NpMbq419/f3bh0AAAAAUF0QbG0mP9+a+/p6tw4AAAAAqC4ItjZDsAUAAAAAdwRbmykosOYEWwAAAACwEGxtxjli68M3BwAAAACSCLa2w6HIAAAAAOCOYGszBFsAAAAAcEewtRmCLQAAAAC4I9jajPPiUZxjCwAAAAAW4pHNMGILAAAAAO4ItjZDsAUAAAAAdwRbmyHYAgAAAIA7gq3NOM+xJdgCAAAAgIVgazPOEVsuHgUAAAAAFuKRzXAoMgAAAAC4I9jaDMEWAAAAANwRbG2GYAsAAAAA7mwXbKdPn67mzZsrKChIXbt21cqVK8v1ujlz5sjhcKhfv35nt8CzjItHAQAAAIA7WwXbuXPnKjExURMmTNCaNWvUvn17JSQkaN++fWW+bseOHXrwwQfVo0ePKqr07OHiUQAAAADgzlbx6LnnntM999yjIUOGqHXr1po5c6ZCQkL0xhtvlPqa/Px8DRw4UP/4xz/UsmXLMrefnZ2trKwst6k6ef556T//sR4zYgsAAAAAFtsE25ycHKWkpCg+Pt61zMfHR/Hx8UpOTi71dU8++aQiIyM1dOjQU77HpEmTFB4e7ppiY2MrpfbKMnp04WOCLQAAAABYbBNsDxw4oPz8fEVFRbktj4qKUlpamsfXLF++XK+//rpee+21cr3HuHHjlJmZ6Zp27dp1xnWfLQRbAAAAALD4ebuAs+XIkSO644479Nprr6lhw4blek1gYKACAwPPcmWVg3NsAQAAAMBim2DbsGFD+fr6Kj093W15enq6oqOjS7T/9ddftWPHDvXt29e1rOD/Lyns5+enLVu26Nxzzz27RZ9FjNgCAAAAgMU2434BAQHq1KmTkpKSXMsKCgqUlJSkuLi4Eu1btWqldevWKTU11TX99a9/1ZVXXqnU1NRqd/5sRRFsAQAAAMBimxFbSUpMTNTgwYPVuXNndenSRdOmTdOxY8c0ZMgQSdKgQYN0zjnnaNKkSQoKClKbNm3cXh8RESFJJZbbEcEWAAAAACy2Crb9+/fX/v37NX78eKWlpalDhw5auHCh64JSO3fulE8tOfmUYAsAAAAAFocxxni7iOoqKytL4eHhyszMVFhYmLfLkcNR+HjaNGnUKK+VAgAAANQq1S0bwF3tGN6sgRixBQAAAAALwdamCLYAAAAAYCHY2kTxA8YJtgAAAABgIdjaRG6u+3OCLQAAAABYCLY2kZPj/ryWXPwZAAAAAE6JeGQTxYMtI7YAAAAAYCHY2gTBFgAAAAA8I9jaBMEWAAAAADwj2NoE59gCAAAAgGfEI5vgqsgAAAAA4BnB1iY4FBkAAAAAPCPY2gTBFgAAAAA8I9jaBMEWAAAAADwj2NoEF48CAAAAAM+IRzbBiC0AAAAAeEawtQmuigwAAAAAnhFsbYIRWwAAAADwjGBrEwRbAAAAAPCMYGsTXDwKAAAAADwjHtkEI7YAAAAA4BnB1iYItgAAAADgGcHWJgi2AAAAAOAZwdYmit/uh3NsAQAAAMBCPLIJRmwBAAAAwDOCrU2MHi3Nnl34nGALAAAAABaCrU2EhkpNmxY+J9gCAAAAgIVgayN+foWPCbYAAAAAYCHY2kjRYMvFowAAAADAQjyyEUZsAQAAAKAkgq2NEGwBAAAAoCSCrY0QbAEAAACgJIKtjRQNswRbAAAAALAQbG2k6AWjuHgUAAAAAFiIRzbicBQ+ZsQWAAAAACwEWxsh2AIAAABASQRbGyl6+HHRkAsAAAAAtRnB1kaCggofE2wBAAAAwOJ36iaoLqKipEcflQIDpdBQb1cDAAAAANUDwdZmnnrK2xUAAAAAQPViu0ORp0+frubNmysoKEhdu3bVypUrS2372muvqUePHqpXr57q1aun+Pj4MtsDAAAAAOzHVsF27ty5SkxM1IQJE7RmzRq1b99eCQkJ2rdvn8f2S5Ys0YABA/Ttt98qOTlZsbGx6tWrl3bv3l3FlQMAAAAAzhaHMcZ4u4jy6tq1qy655BK99NJLkqSCggLFxsbqvvvu09ixY0/5+vz8fNWrV08vvfSSBg0adMr2WVlZCg8PV2ZmpsLCws64fgAAAAD2RDao3mwzYpuTk6OUlBTFx8e7lvn4+Cg+Pl7Jycnl2sbx48eVm5ur+vXre1yfnZ2trKwstwkAAAAAUL3ZJtgeOHBA+fn5ioqKclseFRWltLS0cm3jkUceUUxMjFs4LmrSpEkKDw93TbGxsWdcNwAAAADg7LJNsD1TkydP1pw5czRv3jwFFb0hbBHjxo1TZmama9q1a1cVVwkAAAAAqCjb3O6nYcOG8vX1VXp6utvy9PR0RUdHl/naf//735o8ebK++eYbtWvXrtR2gYGBCgwMdD13nn7MIckAAABA7ebMBDa6RFGtYptgGxAQoE6dOikpKUn9+vWTZF08KikpSSNHjiz1dVOmTNFTTz2lr776Sp07d67Qex45ckSSOCQZAAAAgCQrI4SHh3u7DBRjm2ArSYmJiRo8eLA6d+6sLl26aNq0aTp27JiGDBkiSRo0aJDOOeccTZo0SZL0zDPPaPz48Xr33XfVvHlz17m4derUUZ06dU75fjExMdq1a5fq1q0rh8Nx9nasnLKyshQbG6tdu3ZxJTaUC30GFUWfQUXRZ1BR9BlUVHXpM8YYHTlyRDExMV6rAaWzVbDt37+/9u/fr/HjxystLU0dOnTQwoULXReU2rlzp3x8Ck8bnjFjhnJycnTTTTe5bWfChAmaOHHiKd/Px8dHTZo0qdR9qAxhYWH8Q4AKoc+gougzqCj6DCqKPoOKqg59hpHa6stWwVaSRo4cWeqhx0uWLHF7vmPHjrNfEAAAAADAq2rNVZEBAAAAADUTwdZGAgMDNWHCBLcrNwNloc+gougzqCj6DCqKPoOKos+gPByG61UDAAAAAGyMEVsAAAAAgK0RbAEAAAAAtkawBQAAAADYGsEWAAAAAGBrBFsAAAAAgK0RbG1i+vTpat68uYKCgtS1a1etXLnS2yXBCyZNmqRLLrlEdevWVWRkpPr166ctW7a4tTl58qRGjBihBg0aqE6dOrrxxhuVnp7u1mbnzp3q06ePQkJCFBkZqYceekh5eXlVuSvwksmTJ8vhcGj06NGuZfQZeLJ7927dfvvtatCggYKDg9W2bVutXr3atd4Yo/Hjx6tx48YKDg5WfHy8tm7d6raNQ4cOaeDAgQoLC1NERISGDh2qo0ePVvWuoArk5+friSeeUIsWLRQcHKxzzz1X//znP1X05hv0mdpt2bJl6tu3r2JiYuRwODR//ny39ZXVP37++Wf16NFDQUFBio2N1ZQpU872rqGaINjawNy5c5WYmKgJEyZozZo1at++vRISErRv3z5vl4YqtnTpUo0YMUI//vijFi1apNzcXPXq1UvHjh1ztRkzZow+++wzffDBB1q6dKn27NmjG264wbU+Pz9fffr0UU5Ojn744Qe99dZbmjVrlsaPH++NXUIVWrVqlV555RW1a9fObTl9BsUdPnxY3bp1k7+/v7788ktt3LhRzz77rOrVq+dqM2XKFL3wwguaOXOmVqxYodDQUCUkJOjkyZOuNgMHDtSGDRu0aNEiLViwQMuWLdOwYcO8sUs4y5555hnNmDFDL730kjZt2qRnnnlGU6ZM0YsvvuhqQ5+p3Y4dO6b27dtr+vTpHtdXRv/IyspSr1691KxZM6WkpGjq1KmaOHGiXn311bO+f6gGDKq9Ll26mBEjRrie5+fnm5iYGDNp0iQvVoXqYN++fUaSWbp0qTHGmIyMDOPv728++OADV5tNmzYZSSY5OdkYY8wXX3xhfHx8TFpamqvNjBkzTFhYmMnOzq7aHUCVOXLkiDn//PPNokWLTM+ePc2oUaOMMfQZePbII4+Y7t27l7q+oKDAREdHm6lTp7qWZWRkmMDAQPPee+8ZY4zZuHGjkWRWrVrlavPll18ah8Nhdu/effaKh1f06dPH3HXXXW7LbrjhBjNw4EBjDH0G7iSZefPmuZ5XVv94+eWXTb169dz+bXrkkUfMhRdeeJb3CNUBI7bVXE5OjlJSUhQfH+9a5uPjo/j4eCUnJ3uxMlQHmZmZkqT69etLklJSUpSbm+vWX1q1aqWmTZu6+ktycrLatm2rqKgoV5uEhARlZWVpw4YNVVg9qtKIESPUp08ft74h0Wfg2aeffqrOnTvr5ptvVmRkpDp27KjXXnvNtX779u1KS0tz6zfh4eHq2rWrW7+JiIhQ586dXW3i4+Pl4+OjFStWVN3OoEpcdtllSkpK0i+//CJJ+umnn7R8+XL17t1bEn0GZaus/pGcnKzLL79cAQEBrjYJCQnasmWLDh8+XEV7A2/x83YBKNuBAweUn5/v9oNSkqKiorR582YvVYXqoKCgQKNHj1a3bt3Upk0bSVJaWpoCAgIUERHh1jYqKkppaWmuNp76k3Mdap45c+ZozZo1WrVqVYl19Bl48ttvv2nGjBlKTEzUo48+qlWrVun+++9XQECABg8e7PrePfWLov0mMjLSbb2fn5/q169Pv6mBxo4dq6ysLLVq1Uq+vr7Kz8/XU089pYEDB0oSfQZlqqz+kZaWphYtWpTYhnNd0dMpUPMQbAGbGjFihNavX6/ly5d7uxRUY7t27dKoUaO0aNEiBQUFebsc2ERBQYE6d+6sp59+WpLUsWNHrV+/XjNnztTgwYO9XB2qo/fff1+zZ8/Wu+++q4svvlipqakaPXq0YmJi6DMAqgSHIldzDRs2lK+vb4krlKanpys6OtpLVcHbRo4cqQULFujbb79VkyZNXMujo6OVk5OjjIwMt/ZF+0t0dLTH/uRch5olJSVF+/bt05/+9Cf5+fnJz89PS5cu1QsvvCA/Pz9FRUXRZ1BC48aN1bp1a7dlF110kXbu3Cmp8Hsv69+m6OjoEhc5zMvL06FDh+g3NdBDDz2ksWPH6tZbb1Xbtm11xx13aMyYMZo0aZIk+gzKVln9g3+vajeCbTUXEBCgTp06KSkpybWsoKBASUlJiouL82Jl8AZjjEaOHKl58+Zp8eLFJQ636dSpk/z9/d36y5YtW7Rz505Xf4mLi9O6devc/nFYtGiRwsLCSvyQhf1dddVVWrdunVJTU11T586dNXDgQNdj+gyK69atW4lbif3yyy9q1qyZJKlFixaKjo526zdZWVlasWKFW7/JyMhQSkqKq83ixYtVUFCgrl27VsFeoCodP35cPj7uPyt9fX1VUFAgiT6DslVW/4iLi9OyZcuUm5vrarNo0SJdeOGFHIZcG3j76lU4tTlz5pjAwEAza9Yss3HjRjNs2DATERHhdoVS1A7Dhw834eHhZsmSJWbv3r2u6fjx46429957r2natKlZvHixWb16tYmLizNxcXGu9Xl5eaZNmzamV69eJjU11SxcuNA0atTIjBs3zhu7BC8oelVkY+gzKGnlypXGz8/PPPXUU2br1q1m9uzZJiQkxLzzzjuuNpMnTzYRERHmk08+MT///LO57rrrTIsWLcyJEydcba6++mrTsWNHs2LFCrN8+XJz/vnnmwEDBnhjl3CWDR482JxzzjlmwYIFZvv27ebjjz82DRs2NA8//LCrDX2mdjty5IhZu3atWbt2rZFknnvuObN27Vrz+++/G2Mqp39kZGSYqKgoc8cdd5j169ebOXPmmJCQEPPKK69U+f6i6hFsbeLFF180TZs2NQEBAaZLly7mxx9/9HZJ8AJJHqc333zT1ebEiRPm73//u6lXr54JCQkx119/vdm7d6/bdnbs2GF69+5tgoODTcOGDc0DDzxgcnNzq3hv4C3Fgy19Bp589tlnpk2bNiYwMNC0atXKvPrqq27rCwoKzBNPPGGioqJMYGCgueqqq8yWLVvc2hw8eNAMGDDA1KlTx4SFhZkhQ4aYI0eOVOVuoIpkZWWZUaNGmaZNm5qgoCDTsmVL89hjj7nddoU+U7t9++23Hn/DDB482BhTef3jp59+Mt27dzeBgYHmnHPOMZMnT66qXYSXOYwxxjtjxQAAAAAAnDnOsQUAAAAA2BrBFgAAAABgawRbAAAAAICtEWwBAAAAALZGsAUAAAAA2BrBFgAAAABgawRbAAAAAICtEWwBAAAAALZGsAUAAAAA2BrBFgAAAABgawRbAAAAAICtEWwBAAAAALZGsAUAAAAA2BrBFgAAAABgawRbAAAAAICtEWwBAAAAALZGsAUAAAAA2BrBFgAAAABga37eLqA6Kygo0J49e1S3bl05HA5vlwMAAADAS4wxOnLkiGJiYuTjw/hgdUOwLcOePXsUGxvr7TIAAAAAVBO7du1SkyZNvF0GiiHYlqFu3bqSrM4bFhbm5WoAAAAAeEtWVpZiY2NdGQHVC8G2DM7Dj8PCwgi2AAAAADhFsZri4HAAAAAAgK0RbAEAAAAAtkawBQAAAADYGsEWAAAAAGBrBFsAAAAAgK1xVWS72LNH+v13qVEj6bzzvF0NAAAAAFQbjNjaxZtvSpddJk2a5O1KAAAAAKBaIdjahb+/Nc/N9W4dAAAAAFDNEGztwhls8/K8WwcAAAAAVDMEW7tgxBYAAAAAPCLY2gXBFgAAAAA8ItjaBcEWAAAAADwi2NoFwRYAAAAAPCLY2gXBFgAAAAA8ItjaBcEWAAAAADwi2NoFwRYAAAAAPCLY2gXBFgAAAAA8ItjaBcEWAAAAADwi2NoFwRYAAAAAPCLY2gXBFgAAAAA8ItjaBcEWAAAAADwi2NoFwRYAAAAAPCLY2oUz2OblebcOAAAAAKhmCLZ2wYgtAAAAAHhEsLULgi0AAAAAeESwtQuCLQAAAAB4RLC1i6LB1hjv1gIAAAAA1QjB1i78/Aof5+d7rw4AAAAAqGYItnbhHLGVOBwZAAAAAIog2NoFwRYAAAAAPCLY2kXRYLtxo5ST471aAAAAAKAaIdjaha9v4eO4OKlXL+/VAgAAAADVCMHWLhwO91HbpUu9VwsAAAAAVCMEWzspGmwBAAAAAJIItvZCsAUAAACAEgi2dkKwBQAAAIASCLZ2QrAFAAAAgBIItnZCsAUAAACAEgi2dkKwBQAAAIASCLZ2QrAFAAAAgBIItnZCsAUAAACAEiocbJctW6a+ffsqJiZGDodD8+fPd1t/5513yuFwuE1XX321W5tDhw5p4MCBCgsLU0REhIYOHaqjR4+6tfn555/Vo0cPBQUFKTY2VlOmTClRywcffKBWrVopKChIbdu21RdffOG23hij8ePHq3HjxgoODlZ8fLy2bt1a0V2uPooH27w879QBAAAAANVIhYPtsWPH1L59e02fPr3UNldffbX27t3rmt577z239QMHDtSGDRu0aNEiLViwQMuWLdOwYcNc67OystSrVy81a9ZMKSkpmjp1qiZOnKhXX33V1eaHH37QgAEDNHToUK1du1b9+vVTv379tH79elebKVOm6IUXXtDMmTO1YsUKhYaGKiEhQSdPnqzoblcPxYNtdrZ36gAAAACAasRhjDGn/WKHQ/PmzVO/fv1cy+68805lZGSUGMl12rRpk1q3bq1Vq1apc+fOkqSFCxfqmmuu0R9//KGYmBjNmDFDjz32mNLS0hQQECBJGjt2rObPn6/NmzdLkvr3769jx45pwYIFrm1feuml6tChg2bOnCljjGJiYvTAAw/owQcflCRlZmYqKipKs2bN0q233nrK/cvKylJ4eLgyMzMVFhZ2Oh9R5br8cum77wqfHzwo1a/vvXoAAACAWqLaZQO4OSvn2C5ZskSRkZG68MILNXz4cB08eNC1Ljk5WREREa5QK0nx8fHy8fHRihUrXG0uv/xyV6iVpISEBG3ZskWHDx92tYmPj3d734SEBCUnJ0uStm/frrS0NLc24eHh6tq1q6tNcdnZ2crKynKbqpWgIPfnjNgCAAAAQOUH26uvvlpvv/22kpKS9Mwzz2jp0qXq3bu38vPzJUlpaWmKjIx0e42fn5/q16+vtLQ0V5uoqCi3Ns7np2pTdH3R13lqU9ykSZMUHh7ummJjYyu8/2dVYKD7c7seUg0AAAAAlcivsjdY9BDftm3bql27djr33HO1ZMkSXXXVVZX9dpVq3LhxSkxMdD3PysqqXuGWEVsAAAAAKOGs3+6nZcuWatiwobZt2yZJio6O1r59+9za5OXl6dChQ4qOjna1SU9Pd2vjfH6qNkXXF32dpzbFBQYGKiwszG2qVooHW0ZsAQAAAODsB9s//vhDBw8eVOPGjSVJcXFxysjIUEpKiqvN4sWLVVBQoK5du7raLFu2TLm5ua42ixYt0oUXXqh69eq52iQlJbm916JFixQXFydJatGihaKjo93aZGVlacWKFa42tsOILQAAAACUUOFge/ToUaWmpio1NVWSdZGm1NRU7dy5U0ePHtVDDz2kH3/8UTt27FBSUpKuu+46nXfeeUpISJAkXXTRRbr66qt1zz33aOXKlfr+++81cuRI3XrrrYqJiZEk3XbbbQoICNDQoUO1YcMGzZ07V88//7zbYcKjRo3SwoUL9eyzz2rz5s2aOHGiVq9erZEjR0qyrtg8evRo/etf/9Knn36qdevWadCgQYqJiXG7irOtMGILAAAAACVU+Bzb1atX68orr3Q9d4bNwYMHa8aMGfr555/11ltvKSMjQzExMerVq5f++c9/KrDIhY9mz56tkSNH6qqrrpKPj49uvPFGvfDCC6714eHh+vrrrzVixAh16tRJDRs21Pjx493udXvZZZfp3Xff1eOPP65HH31U559/vubPn682bdq42jz88MM6duyYhg0bpoyMDHXv3l0LFy5UUPGAaBeM2AIAAABACWd0H9uartrdq+qJJ6R//avw+aefSn37eq8eAAAAoJaodtkAbs76ObaoRMVHbH/+2Tt1AAAAAEA1QrC1k+LB9vHHpUOHvFMLAAAAAFQTBFs78XRu8B9/VH0dAAAAAFCNEGztxFOwLSio+joAAAAAoBoh2NqJp2DLlZEBAAAA1HIEWzsh2AIAAABACQRbOyHYAgAAAEAJBFs7KRpsQ0OtOcEWAAAAQC1HsLWTosG2Th1rTrAFAAAAUMsRbO2kaLCtW9ea5+R4pxYAAAAAqCYItnbCiC0AAAAAlECwtZOAgMLHBFsAAAAAkESwtRdf38LHBFsAAAAAkESwtRefIl9XSIg1J9gCAAAAqOX8vF0AKuCcc6TwcCkwUKpXz1pGsAUAAABQyzFiayf+/lJamrRzZ+GFpAi2AAAAAGo5RmztxhloAwOtOcEWAAAAQC3HiK1dOYMt97EFAAAAUMsRbO2KEVsAAAAAkESwtS+CLQAAAABIItjaF8EWAAAAACQRbO2LYAsAAAAAkgi29kWwBQAAAABJBFv7Cgiw5gRbAAAAALUcwdauGLEFAAAAAEkEW/viPrYAAAAAIIlga1+M2AIAAACAJIKtfRFsAQAAAEASwda+goKs+cmT3q0DAAAAALyMYGtXwcHW/MQJ79YBAAAAAF5GsLUrZ7A9fty7dQAAAACAlxFs7SokxJozYgsAAACgliPY2pVzxDYnR8rP924tAAAAAOBFBFu7cgZbiVFbAAAAALUawdauCLYAAAAAIIlga18+PoX3siXYAgAAAKjFCLZ2xpWRAQAAAIBga2vcyxYAAAAACLa2xi1/AAAAAIBga2uM2AIAAAAAwdbWOMcWAAAAACoebJctW6a+ffsqJiZGDodD8+fPd1tvjNH48ePVuHFjBQcHKz4+Xlu3bnVrc+jQIQ0cOFBhYWGKiIjQ0KFDdfToUbc2P//8s3r06KGgoCDFxsZqypQpJWr54IMP1KpVKwUFBalt27b64osvKlyLrXEoMgAAAABUPNgeO3ZM7du31/Tp0z2unzJlil544QXNnDlTK1asUGhoqBISEnTy5ElXm4EDB2rDhg1atGiRFixYoGXLlmnYsGGu9VlZWerVq5eaNWumlJQUTZ06VRMnTtSrr77qavPDDz9owIABGjp0qNauXat+/fqpX79+Wr9+fYVqsTUORQYAAAAAOYwx5rRf7HBo3rx56tevnyRrhDQmJkYPPPCAHnzwQUlSZmamoqKiNGvWLN16663atGmTWrdurVWrVqlz586SpIULF+qaa67RH3/8oZiYGM2YMUOPPfaY0tLSFBAQIEkaO3as5s+fr82bN0uS+vfvr2PHjmnBggWuei699FJ16NBBM2fOLFctp5KVlaXw8HBlZmYqLCzsdD+ms+eGG6R586SXX5aGD/d2NQAAAECNVe2zQS1XqefYbt++XWlpaYqPj3ctCw8PV9euXZWcnCxJSk5OVkREhCvUSlJ8fLx8fHy0YsUKV5vLL7/cFWolKSEhQVu2bNHhw4ddbYq+j7ON833KU0tx2dnZysrKcpuqNUZsAQAAAKByg21aWpokKSoqym15VFSUa11aWpoiIyPd1vv5+al+/fpubTxto+h7lNam6PpT1VLcpEmTFB4e7ppiY2PLsddexDm2AAAAAMBVkYsaN26cMjMzXdOuXbu8XVLZuCoyAAAAAFRusI2OjpYkpaenuy1PT093rYuOjta+ffvc1ufl5enQoUNubTxto+h7lNam6PpT1VJcYGCgwsLC3KZqzTliS7AFAAAAUItVarBt0aKFoqOjlZSU5FqWlZWlFStWKC4uTpIUFxenjIwMpaSkuNosXrxYBQUF6tq1q6vNsmXLlJub62qzaNEiXXjhhapXr56rTdH3cbZxvk95arE9Z/Cu7ucCAwAAAMBZVOFge/ToUaWmpio1NVWSdZGm1NRU7dy5Uw6HQ6NHj9a//vUvffrpp1q3bp0GDRqkmJgY15WTL7roIl199dW65557tHLlSn3//fcaOXKkbr31VsXExEiSbrvtNgUEBGjo0KHasGGD5s6dq+eff16JiYmuOkaNGqWFCxfq2Wef1ebNmzVx4kStXr1aI0eOlKRy1WJ7BFsAAAAAkF9FX7B69WpdeeWVrufOsDl48GDNmjVLDz/8sI4dO6Zhw4YpIyND3bt318KFCxUUFOR6zezZszVy5EhdddVV8vHx0Y033qgXXnjBtT48PFxff/21RowYoU6dOqlhw4YaP368271uL7vsMr377rt6/PHH9eijj+r888/X/Pnz1aZNG1eb8tRia+Hh1jwz07t1AAAAAIAXndF9bGu6an+vqk8+kfr1k7p2lX780dvVAAAAADVWtc8GtRxXRbYzRmwBAAAAgGBra5xjCwAAAAAEW1tjxBYAAAAACLa25hyxPXZMys/3bi0AAAAA4CUEWzsretL6kSPeqwMAAAAAvIhga2eBgdYkcTgyAAAAgFqLYGt3zvNsn3hCatNG2rHDq+UAAAAAQFUj2Nqd83Dk//1P2rBB+vRT79YDAAAAAFWMYGt3l17q/vzAAe/UAQAAAABeQrC1u4kTpZCQwufp6V4rBQAAAAC8gWBrd+eeK23eLP3zn9bztDTv1gMAAAAAVYxgWxPExloXjpIYsQUAAABQ6xBsa4qoKGvOiC0AAACAWoZgW1NER1vz9HTJGO/WAgAAAABViGBbUzhHbE+elLKyvFsLAAAAAFQhgm1NERIiBQVZjw8f9m4tAAAAAFCFCLY1SXi4Nc/M9G4dAAAAAFCFCLY1SUSENc/I8GYVAAAAAFClCLY1CSO2AAAAAGohgm1NQrAFAAAAUAsRbGsSDkUGAAAAUAsRbGsSRmwBAAAA1EIE25rEOWJLsAUAAABQixBsaxLniC2HIgMAAACoRQi2NQmHIgMAAACohQi2NQmHIgMAAACohQi2NUm9etb84EEpKUnatMm79QAAAABAFfDzdgGoRLGx1jwlRYqPtx4b4716AAAAAKAKMGJbkzRvXnJZWlqVlwEAAAAAVYlgW5OEhxcejuyUmuqVUgAAAACgqhBsa5qmTd2fE2wBAAAA1HAE25rG19f9OReQAgAAAFDDEWxrmsREKSxMuuYa6/nevd6tBwAAAADOMoJtTTNwoJSRIY0ebT3fs8eb1QAAAADAWUewrYkcDqlxY+sxI7YAAAAAajiCbU3lDLaHDknZ2d6tBQAAAADOIoJtTVW/vhQQYD3mXrYAAAAAajCCbU3lcEjR0dZjDkcGAAAAUIMRbGuyoufZrl4tLVrk3XoAAAAA4Cwg2NZkMTHWfM8e6ZJLpF69pJ9+8m5NAAAAAFDJCLY1mXPEdtOmwmVffOGdWgAAAADgLCHY1mTOYLtqVeGyJUu8UgoAAAAAnC2VHmwnTpwoh8PhNrVq1cq1/uTJkxoxYoQaNGigOnXq6MYbb1R6errbNnbu3Kk+ffooJCREkZGReuihh5SXl+fWZsmSJfrTn/6kwMBAnXfeeZo1a1aJWqZPn67mzZsrKChIXbt21cqVKyt7d6s3T8E2KYmLSQEAAACoUc7KiO3FF1+svXv3uqbly5e71o0ZM0afffaZPvjgAy1dulR79uzRDTfc4Fqfn5+vPn36KCcnRz/88IPeeustzZo1S+PHj3e12b59u/r06aMrr7xSqampGj16tO6++2599dVXrjZz585VYmKiJkyYoDVr1qh9+/ZKSEjQvn37zsYuV0/OYGtM4bL8fOm997xTDwAAAACcBQ5jiqaeMzdx4kTNnz9fqampJdZlZmaqUaNGevfdd3XTTTdJkjZv3qyLLrpIycnJuvTSS/Xll1/q2muv1Z49exQVFSVJmjlzph555BHt379fAQEBeuSRR/T5559r/fr1rm3feuutysjI0MKFCyVJXbt21SWXXKKXXnpJklRQUKDY2Fjdd999Gjt2bLn2JSsrS+Hh4crMzFRYWNiZfCzekZoqdexYcvmwYdIrr1R5OQAAAIBd2T4b1HBnZcR269atiomJUcuWLTVw4EDt3LlTkpSSkqLc3FzFx8e72rZq1UpNmzZVcnKyJCk5OVlt27Z1hVpJSkhIUFZWljZs2OBqU3QbzjbObeTk5CglJcWtjY+Pj+Lj411tPMnOzlZWVpbbZGvOqyI7Oe9re+BA1dcCAAAAAGdJpQfbrl27atasWVq4cKFmzJih7du3q0ePHjpy5IjS0tIUEBCgiIgIt9dERUUpLS1NkpSWluYWap3rnevKapOVlaUTJ07owIEDys/P99jGuQ1PJk2apPDwcNcUGxt7Wp9BtdGokdSkSeHzyy+35vv3e6ceAAAAADgLKj3Y9u7dWzfffLPatWunhIQEffHFF8rIyND7779f2W9V6caNG6fMzEzXtGvXLm+XdGYcDqlp08LnAwZYc4ItAAAAgBrkrN/uJyIiQhdccIG2bdum6Oho5eTkKCMjw61Nenq6ov//MNno6OgSV0l2Pj9Vm7CwMAUHB6thw4by9fX12Ma5DU8CAwMVFhbmNtnewIHWvGVL6bzzrMcEWwAAAAA1yFkPtkePHtWvv/6qxo0bq1OnTvL391dSUpJr/ZYtW7Rz507FxcVJkuLi4rRu3Tq3qxcvWrRIYWFhat26tatN0W042zi3ERAQoE6dOrm1KSgoUFJSkqtNrXHvvdKsWdK331qHJkvSoUPW1ZEBAAAAoAbwq+wNPvjgg+rbt6+aNWumPXv2aMKECfL19dWAAQMUHh6uoUOHKjExUfXr11dYWJjuu+8+xcXF6dJLL5Uk9erVS61bt9Ydd9yhKVOmKC0tTY8//rhGjBihwMBASdK9996rl156SQ8//LDuuusuLV68WO+//74+//xzVx2JiYkaPHiwOnfurC5dumjatGk6duyYhgwZUtm7XL35+EiDB1uPnfcCNkY6eFCKjPReXQAAAABQSSo92P7xxx8aMGCADh48qEaNGql79+768ccf1ej/Rwv/85//yMfHRzfeeKOys7OVkJCgl19+2fV6X19fLViwQMOHD1dcXJxCQ0M1ePBgPfnkk642LVq00Oeff64xY8bo+eefV5MmTfTf//5XCQkJrjb9+/fX/v37NX78eKWlpalDhw5auHBhiQtK1Sp+flL9+taI7f79BFsAAAAANUKl38e2JqmR96pq3VratEn6/HPpmmu8XQ0AAABgCzUyG9QgZ/0cW1QzXbpY82XLvFsHAAAAAFQSgm1t8+c/W/MlS7xaBgAAAABUFoJtbeMMtqtWSQcOWI+HDbNuBfTMM1JmpvdqAwAAAIDTQLCtbZo0kTp2lAoKpE8+kXbvll57Tfr1V2nsWOmf//R2hQAAAABQIQTb2qhfP2v+t79JsbHu61atqvJyAAAAAOBMEGxro/vuk9q2lfLzrXvaStI991jzjRu9VxcAAAAAnAaCbW1Ur5705ZdSixZSdLQ0b570n/9IDod13u2+fd6uEAAAAADKzc/bBcBLzjlH2rzZOtc2KMha1qKF9Ntv0rp10lVXebc+AAAAACgnRmxrs4CAwlArSZddZs0XLPBOPQAAAABwGgi2KHTTTdb8ww+tkVwAAAAAsAGCLQolJEh160p//CH9+KO3qwEAAACAciHYolBQkPTXv1qP33rLu7UAAAAAQDkRbOFu8GBr/uqr0rvvercWAAAAACgHgi3c/eUv0kMPWY8HDZKeeELKzvZuTQAAAABQBoItSpo0yRq5zc+X/vUvqXNnaelSyZiyX5edbb0GAAAAAKoQwRYl+fpKs2ZJH3wgNWokrV8vXXGFdOml0ksvSbt2FbY1RvruO+nWW6U6daz2w4ZJx497q3oAAAAAtYzDmFMNw9VeWVlZCg8PV2ZmpsLCwrxdjnccOCA9+qj0zjvSiROFyxs3lho2lPbutdoUd++90owZVVcnAAAAcBaRDao3RmxRtoYNrQtJbdsm/fvfUrt21oju3r3SunVWqA0Olu6+W0pJkd5/X3I4pJkzrfvhAgAAAMBZxohtGfirTCkOHpS2bpXS0qQmTaQLLpCKfj5jx0rPPCOFhko//GCFYQAAAMDGyAbVG8G2DHTe05SXJ119tZSUZI34PvCANHy4FB7u7coAAACA00I2qN4ItmWg856BQ4eknj2tC09JUt260jXXSF27WlPHjtYhzAAAAIANkA2qN4JtGei8Zyg3V5ozxzosecMG93V+ftYhyl26WFPXrlKrVpIPp30DAACg+iEbVG8E2zLQeSuJMdLy5dZtgVassKb09JLt6taVLrnEmtq0kS6+WLrwQikkpOprBgAAAIogG1RvBNsy0HnPEmOse+GuWCGtXGnNU1I83/vW4ZBatLBGc1u2tB47p+bNpYiIqq4eAAAAtRDZoHoj2JaBzluF8vKsw5VXrrRC7saN1vNDh8p+XUSEe9h1Bl7nnNFeAAAAVAKyQfVGsC0DndfLjJH277dC7pYt0vbthdOOHda6U4mKcg+7zqlZMykmxrolEQAAAHAKZIPqjWBbBjpvNXf0qBVwi4bdouE3K+vU2wgLk845xwq5TZoUPo6KsqboaGtet651WDQAAABqJbJB9ebn7QKA01anjnWRqTZtSq4zRjp82HPg3b7dOsf32DEr/GZlSZs2lf1eQUGFYTcy0poaNSp83KCBVL++VK9e4eTHf14AAABAVeCXN2omh8MKmvXrS506lVxvjHTkiLRnj7R7t/u0Z4911WbndPSodPKk9Pvv1lRedeu6B93iwbe05+Hhkq9v5X0WAAAAQA1HsEXt5HBYhyGHhVlXXC7L8eOFIXffvpJTerp1kavDh63JeQj0kSPWtHNnxWsLDy9fCK5b1xq5Lj4PCuLQaQAAANQaBFvgVEJCCi86VR55eVJGRmHQLRp6iz8vvu7YMWs0OSPDmrZvP72afX1LBt7SQnBZc+fjOnUkH5/TqwUAAAA4ywi2QGXz85MaNrSmisrJKX8IPnzYGhE+erRw7rwXcH6+lJlpTZUlJKTsEBwSYk3Bwac355xkAAAAnCZ+SQLVSUBA4UWqTkd+vjXqWzzwFp2Xtc5T24ICa9vHjxceln02+PmdfjAubV1gYMkpKKjwMWEaAACgRuBXHVCT+PoWnjtcGYyxLpxVnpB8/Lh04kTJuadlRedOeXmFV6muKj4+ngNvaUH4bC/ncG8AAIDTQrAFUDqHo3BEtFGjyt++MVJ2dtnB90zm2dmF08mT1tw5Ai1Zj53huzrw8zt1EHZO/v7WCL9zKvq8vOtOtx1X7QYAANUMwRaA9zgcVoALCqq698zL8xx4i08VXX662ypeW16edTh5deZwWGHX398K487HFZlO53UVeY2fnxXA/fwKp/I+54riAADYDsEWQO3iDDGhod6uxBqxzs2teEDOzbUuNJaT4/64+PPTaedpXV5eybqd62siH58zC8aenlfmtqr6OYfIAwBsgGALAN7icBQe3lu3rrerKV1BgRVwiwfe0qa8vLLXV3Q6ne3l5xeOgJf2uKz9ramh/XQ4HFUT0n19Cycfn9Obn8lrK3MbFd0WRwkAwBkj2AIAylb0Ils1hTFWgC0adIsH37JC8Zk+ry7bKvq46PnnxT+rU/0xAGfG4ai5ob26/UHCOTk/8+JTWcsBVGsEWwBA7eMchfT1tUbMYQVYbwfs/PzCPziUd346rzkb2yhtW8aU/7PPzz/73zNO3+mE4Yosr8xt1ZT3KLruz3+u3kc3wesItgAAwPrx6Dw8GJWn6NEBNS20e3tbFW3r/C7K+wcHT98lf4Dwno0bpYsu8nYVqMZqxb9e06dP19SpU5WWlqb27dvrxRdfVJcuXbxdFgAAqOmKHh2A6qVo0C06lba8rHXe3FZteY+QEG/3GFRzNT7Yzp07V4mJiZo5c6a6du2qadOmKSEhQVu2bFFkZKS3ywMAAIA38EcHoEbx8XYBZ9tzzz2ne+65R0OGDFHr1q01c+ZMhYSE6I033vB2aQAAAACASlCjg21OTo5SUlIUHx/vWubj46P4+HglJyeXaJ+dna2srCy3CQAAAABQvdXoYHvgwAHl5+crKirKbXlUVJTS0tJKtJ80aZLCw8NdU2xsbFWVCgAAAAA4TTU62FbUuHHjlJmZ6Zp27drl7ZIAAAAAAKdQoy8e1bBhQ/n6+io9Pd1teXp6uqKjo0u0DwwMVGBgYFWVBwAAAACoBDV6xDYgIECdOnVSUlKSa1lBQYGSkpIUFxfnxcoAAAAAAJWlRo/YSlJiYqIGDx6szp07q0uXLpo2bZqOHTumIUOGnPK15v9v3s1FpAAAAIDazZkJnBkB1UuND7b9+/fX/v37NX78eKWlpalDhw5auHBhiQtKeXLkyBFJ4iJSAAAAACRZGSE8PNzbZaAYh+FPDqUqKCjQnj17VLduXTkcDm+Xo6ysLMXGxmrXrl0KCwvzdjmwAfoMKoo+g4qiz6Ci6DOoqOrSZ4wxOnLkiGJiYuTjU6PP6LSlGj9ieyZ8fHzUpEkTb5dRQlhYGP8QoELoM6go+gwqij6DiqLPoKKqQ59hpLb64k8NAAAAAABbI9gCAAAAAGyNYGsjgYGBmjBhAvfaRbnRZ1BR9BlUFH0GFUWfQUXRZ1AeXDwKAAAAAGBrjNgCAAAAAGyNYAsAAAAAsDWCLQAAAADA1gi2AAAAAABbI9gCAAAAAGyNYGsT06dPV/PmzRUUFKSuXbtq5cqV3i4JXjBp0iRdcsklqlu3riIjI9WvXz9t2bLFrc3Jkyc1YsQINWjQQHXq1NGNN96o9PR0tzY7d+5Unz59FBISosjISD300EPKy8uryl2Bl0yePFkOh0OjR492LaPPwJPdu3fr9ttvV4MGDRQcHKy2bdtq9erVrvXGGI0fP16NGzdWcHCw4uPjtXXrVrdtHDp0SAMHDlRYWJgiIiI0dOhQHT16tKp3BVUgPz9fTzzxhFq0aKHg4GCde+65+uc//6miN9+gz9Ruy5YtU9++fRUTEyOHw6H58+e7ra+s/vHzzz+rR48eCgoKUmxsrKZMmXK2dw3VBMHWBubOnavExERNmDBBa9asUfv27ZWQkKB9+/Z5uzRUsaVLl2rEiBH68ccftWjRIuXm5qpXr146duyYq82YMWP02Wef6YMPPtDSpUu1Z88e3XDDDa71+fn56tOnj3JycvTDDz/orbfe0qxZszR+/Hhv7BKq0KpVq/TKK6+oXbt2bsvpMyju8OHD6tatm/z9/fXll19q48aNevbZZ1WvXj1XmylTpuiFF17QzJkztWLFCoWGhiohIUEnT550tRk4cKA2bNigRYsWacGCBVq2bJmGDRvmjV3CWfbMM89oxowZeumll7Rp0yY988wzmjJlil588UVXG/pM7Xbs2DG1b99e06dP97i+MvpHVlaWevXqpWbNmiklJUVTp07VxIkT9eqrr571/UM1YFDtdenSxYwYMcL1PD8/38TExJhJkyZ5sSpUB/v27TOSzNKlS40xxmRkZBh/f3/zwQcfuNps2rTJSDLJycnGGGO++OIL4+PjY9LS0lxtZsyYYcLCwkx2dnbV7gCqzJEjR8z5559vFi1aZHr27GlGjRpljKHPwLNHHnnEdO/evdT1BQUFJjo62kydOtW1LCMjwwQGBpr33nvPGGPMxo0bjSSzatUqV5svv/zSOBwOs3v37rNXPLyiT58+5q677nJbdsMNN5iBAwcaY+gzcCfJzJs3z/W8svrHyy+/bOrVq+f2b9MjjzxiLrzwwrO8R6gOGLGt5nJycpSSkqL4+HjXMh8fH8XHxys5OdmLlaE6yMzMlCTVr19fkpSSkqLc3Fy3/tKqVSs1bdrU1V+Sk5PVtm1bRUVFudokJCQoKytLGzZsqMLqUZVGjBihPn36uPUNiT4Dzz799FN17txZN998syIjI9WxY0e99tprrvXbt29XWlqaW78JDw9X165d3fpNRESEOnfu7GoTHx8vHx8frVixoup2BlXisssuU1JSkn755RdJ0k8//aTly5erd+/ekugzKFtl9Y/k5GRdfvnlCggIcLVJSEjQli1bdPjw4SraG3iLn7cLQNkOHDig/Px8tx+UkhQVFaXNmzd7qSpUBwUFBRo9erS6deumNm3aSJLS0tIUEBCgiIgIt7ZRUVFKS0tztfHUn5zrUPPMmTNHa9as0apVq0qso8/Ak99++00zZsxQYmKiHn30Ua1atUr333+/AgICNHjwYNf37qlfFO03kZGRbuv9/PxUv359+k0NNHbsWGVlZalVq1by9fVVfn6+nnrqKQ0cOFCS6DMoU2X1j7S0NLVo0aLENpzrip5OgZqHYAvY1IgRI7R+/XotX77c26WgGtu1a5dGjRqlRYsWKSgoyNvlwCYKCgrUuXNnPf3005Kkjh07av369Zo5c6YGDx7s5epQHb3//vuaPXu23n33XV188cVKTU3V6NGjFRMTQ58BUCU4FLmaa9iwoXx9fUtcoTQ9PV3R0dFeqgreNnLkSC1YsEDffvutmjRp4loeHR2tnJwcZWRkuLUv2l+io6M99ifnOtQsKSkp2rdvn/70pz/Jz89Pfn5+Wrp0qV544QX5+fkpKiqKPoMSGjdurNatW7stu+iii7Rz505Jhd97Wf82RUdHl7jIYV5eng4dOkS/qYEeeughjR07Vrfeeqvatm2rO+64Q2PGjNGkSZMk0WdQtsrqH/x7VbsRbKu5gIAAderUSUlJSa5lBQUFSkpKUlxcnBcrgzcYYzRy5EjNmzdPixcvLnG4TadOneTv7+/WX7Zs2aKdO3e6+ktcXJzWrVvn9o/DokWLFBYWVuKHLOzvqquu0rp165SamuqaOnfurIEDB7oe02dQXLdu3UrcSuyXX35Rs2bNJEktWrRQdHS0W7/JysrSihUr3PpNRkaGUlJSXG0WL16sgoICde3atQr2AlXp+PHj8vFx/1np6+urgoICSfQZlK2y+kdcXJyWLVum3NxcV5tFixbpwgsv5DDk2sDbV6/Cqc2ZM8cEBgaaWbNmmY0bN5phw4aZiIgItyuUonYYPny4CQ8PN0uWLDF79+51TcePH3e1uffee03Tpk3N4sWLzerVq01cXJyJi4tzrc/LyzNt2rQxvXr1MqmpqWbhwoWmUaNGZty4cd7YJXhB0asiG0OfQUkrV640fn5+5qmnnjJbt241s2fPNiEhIeadd95xtZk8ebKJiIgwn3zyifn555/NddddZ1q0aGFOnDjhanP11Vebjh07mhUrVpjly5eb888/3wwYMMAbu4SzbPDgweacc84xCxYsMNu3bzcff/yxadiwoXn44YddbegztduRI0fM2rVrzdq1a40k89xzz5m1a9ea33//3RhTOf0jIyPDREVFmTvuuMOsX7/ezJkzx4SEhJhXXnmlyvcXVY9gaxMvvviiadq0qQkICDBdunQxP/74o7dLghdI8ji9+eabrjYnTpwwf//73029evVMSEiIuf76683evXvdtrNjxw7Tu3dvExwcbBo2bGgeeOABk5ubW8V7A28pHmzpM/Dks88+M23atDGBgYGmVatW5tVXX3VbX1BQYJ544gkTFRVlAgMDzVVXXWW2bNni1ubgwYNmwIABpk6dOiYsLMwMGTLEHDlypCp3A1UkKyvLjBo1yjRt2tQEBQWZli1bmscee8zttiv0mdrt22+/9fgbZvDgwcaYyusfP/30k+nevbsJDAw055xzjpk8eXJV7SK8zGGMMd4ZKwYAAAAA4Mxxji0AAAAAwNYItgAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABsjWALAAAAALA1gi0AAAAAwNYItgAAAAAAWyPYAgAAAABs7f8ACeJTD785TtIAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "On CPU:                      Training completes in 40m 4.68s\n",
        "On GPU (with matmul kernel): Training completes in 3m 49.59s\n",
        "\n",
        "Speedup: 10.47x faster\n",
        "\"\"\"\n",
        "\n",
        "#GPU: NVIDIA Tesla T4\n",
        "#Cuda matmul kernels launched in lines 104, 178, 180\n",
        "#TO DO: (Once openml finally works again)\n",
        "  # Implement vector addition kernel for adding result of matmul to bias vectors.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "#get mnist\n",
        "print(\"Loading MNIST dataset...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert to torch tensors and move to GPU\n",
        "X_train = train_dataset.data.view(-1, 28*28).float().cuda() / 255.0\n",
        "y_train = train_dataset.targets.long() # Keep as long for one-hot encoding\n",
        "X_test = test_dataset.data.view(-1, 28*28).float().cuda() / 255.0\n",
        "y_test = test_dataset.targets.long() # Keep as long for one-hot encoding\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "\n",
        "\n",
        "class FNN_classifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "\n",
        "    def initializeParams(self, layerDims):\n",
        "        #initialize weights and biases of the network given array of dimensions of each layer\n",
        "        torch.manual_seed(42) # Use torch manual_seed for reproducibility\n",
        "        for i in range(1,len(layerDims)):\n",
        "            self.params['W' + str(i)] = (torch.randn(layerDims[i],layerDims[i-1])*0.1).float().cuda() # Initialize as tensors on GPU\n",
        "            self.params['b' + str(i)] = torch.zeros((layerDims[i],1)).float().cuda() # Initialize as tensors on GPU\n",
        "\n",
        "        return self.params\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    Activation functions\n",
        "    \"\"\"\n",
        "    #Default value, can be changed\n",
        "    activation = \"relu\"\n",
        "\n",
        "    #functions return the value of the function given input Z and returns Z for caching\n",
        "    def relu(self,Z):\n",
        "        return torch.max(torch.tensor(0.0).cuda(),Z), Z # Use torch.max\n",
        "        #return np.maximum(0,Z), Z\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return 1/(1+torch.exp(-Z)), Z # Use torch.exp\n",
        "        #return 1/(1+np.exp(-Z)), Z\n",
        "\n",
        "    def tanh(self,Z):\n",
        "        return torch.tanh(Z), Z # Use torch.tanh\n",
        "        #return np.tanh(Z), Z\n",
        "\n",
        "\n",
        "    def softmax(self,Z):\n",
        "        \"\"\"\n",
        "        ex. for a 3 element vector:\n",
        "        Z\n",
        "        \"\"\"\n",
        "        zexp = torch.exp(Z - torch.max(Z,axis=0,keepdims=True).values) # Use torch.exp and torch.max\n",
        "        return zexp/torch.sum(zexp, axis=0, keepdims = True), Z # Use torch.sum\n",
        "        #zexp = np.exp(Z - np.max(Z,axis=0,keepdims=True)) #Apparently this is a trick that prevents overflow\n",
        "                                    #This will automatically make it so that e^largest value = e^0 = 1\n",
        "                                    #This is also mathematically equivalent to regular softmax\n",
        "        #return zexp/np.sum(zexp, axis=0, keepdims = True), Z\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "    def forwardProp(self,X):\n",
        "        X = X.T\n",
        "        A = X\n",
        "        caches = [] #stores tuples of the linear and activation caches for each layer of the network\n",
        "\n",
        "        L = len(self.params)//2 #self.params is twice as long as the number of layers due to W_i and b_i\n",
        "\n",
        "        for i in range(1, L+1):\n",
        "            prevA = A.contiguous()\n",
        "            #Z = Wx + b - \"linear hypothesis\"\n",
        "            Z = launch_cuda_matmul(self.params['W' + str(i)], prevA) + self.params['b' + str(i)]            # CUDA REPLACEMENT ------------------------------------------\n",
        "\n",
        "            #Caching important values from linear hypothesis\n",
        "            linearCache = (prevA, self.params['W' + str(i)], self.params['b' + str(i)])\n",
        "\n",
        "            #Caching important values from activation. Activation function returns the\n",
        "            #value of the function itself and the input Z, the result of the linear hypothesis\n",
        "\n",
        "            func = 0\n",
        "            if i == L: #This is the final layer, where softmax is implemented.\n",
        "                func = self.softmax(Z)\n",
        "                #print(f\"final layer reached, softmaxed. {L}\")\n",
        "            elif self.activation == \"relu\":\n",
        "                #print(f\"relu'd{L}\")\n",
        "                func = self.relu(Z)\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                func = self.sigmoid(Z)\n",
        "            elif self.activation == \"tanh\":\n",
        "                func = self.tanh(Z)\n",
        "\n",
        "            A, activationCache = func\n",
        "\n",
        "            cache = (linearCache,activationCache)\n",
        "            caches.append(cache)\n",
        "\n",
        "        return A, caches\n",
        "\n",
        "\n",
        "    #Now that forward propagation is done, the loss function needs to be defined\n",
        "    def crossEntropyLoss(self, yTrue, yPred):\n",
        "        yTrue = yTrue.T\n",
        "        \"\"\"\n",
        "        Computes cost function for general cross entropy loss, used for multiclass classification\n",
        "        \"\"\"\n",
        "        #accounts for floating point inaccuracies, keeps values between 0 and 1\n",
        "        yPred = torch.clamp(yPred, 1e-15, 1-1e-15) # Use torch.clamp\n",
        "\n",
        "        #returns a vector of the individual sample losses\n",
        "\n",
        "        losses = -torch.sum(yTrue*torch.log(yPred),axis=0) # Use torch.sum and torch.log\n",
        "\n",
        "        #Determines total cost by taking the average of the loss across the entire dataset.\n",
        "        cost = torch.mean(losses) # Use torch.mean\n",
        "        return cost\n",
        "\n",
        "\n",
        "    #Backprop helper function that runs it for a single layer\n",
        "    def stepBack(self, dA, cache, isOutput = False):\n",
        "        #dA is the gradient of the loss with respect to A, dL/dA\n",
        "        #unpacking cache\n",
        "        linearCache, activationCache = cache\n",
        "        prevA, W, b = linearCache\n",
        "        Z = activationCache\n",
        "        m = prevA.shape[1] #gets number of samples\n",
        "\n",
        "\n",
        "        if isOutput:\n",
        "            dZ = dA\n",
        "        else:\n",
        "            if self.activation == \"relu\":\n",
        "                dZ = dA * (Z>0).float() # Use torch comparison and convert to float\n",
        "                #Interestingly enough, you can multiply numbers by booleans in python.\n",
        "                #This multiplies dA by the derivative of relu which is 1 if positive and 0 if not.\n",
        "            elif self.activation == \"tanh\":\n",
        "                dZ = dA * (1-torch.tanh(Z)**2) # Use torch.tanh\n",
        "                #for y = tanh(x), dy/dx = 1-tanh(x)^2\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                dZ = dA * self.sigmoid(Z)[0] * (1 - self.sigmoid(Z)[0]) # Use sigmoid output\n",
        "                #dZ = dA*self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "                #for y = sigmoid(x), dy/dx = sigmoid(x)*(1-sigmoid(x))\n",
        "            else:\n",
        "                raise ValueError(\"Activation function not supported\")\n",
        "\n",
        "        #Now, we can compute gradients for W, b, and prevA\n",
        "        dW = (1/m)*launch_cuda_matmul(dZ.contiguous(), prevA.T.contiguous())                              # CUDA REPLACEMENT ------------------------------------------\n",
        "        db = (1/m)*torch.sum(dZ, axis=1, keepdims = True) # Use torch.sum\n",
        "        prevdA = launch_cuda_matmul(W.T.contiguous(), dZ.contiguous())                                    # CUDA REPLACEMENT ------------------------------------------\n",
        "\n",
        "        return prevdA, dW, db\n",
        "\n",
        "    #Now that the stepback function is defined for each layer going backwards, we can do backprop\n",
        "    def backprop(self, AL, yPred, caches):\n",
        "        grads = {}\n",
        "        L = len(caches)\n",
        "\n",
        "        m = AL.shape[1]\n",
        "        #Remember, AL is the final activations for every sample represented as a matrix of column vectors\n",
        "        '''\n",
        "        ex.      [p0a p0b p0c p0d p0e]\n",
        "                 [p1a p1b p1c p1d p1e]\n",
        "                 [p2a p2b p2c p2d p2e]\n",
        "                 [p3a p3b p3c p3d p3e]\n",
        "                 [p4a p4b p4c p4d p4e]\n",
        "        Where each final letter of a term represents a different sample. AL.shape[1] returns the number of columns,\n",
        "        also the number of samples.\n",
        "        '''\n",
        "        yPred = yPred.T\n",
        "\n",
        "        dAL = AL - yPred\n",
        "        #Gradient of the loss function with respect to AL for general cross entropy loss.\n",
        "\n",
        "        currentCache = caches[L-1]\n",
        "        grads['dA' + str(L-1)], grads['dW' + str(L-1)], grads['db' + str(L-1)] = self.stepBack(dAL, currentCache, isOutput=True)\n",
        "\n",
        "        for i in reversed(range(L-1)):\n",
        "            currentCache = caches[i]\n",
        "            prevdAtemp, dWtemp, dbtemp = self.stepBack(grads['dA' + str(i+1)], currentCache)\n",
        "\n",
        "            grads['dA' + str(i)] = prevdAtemp\n",
        "            #i+1 is used in order to match up the dA gradient used to calculate the weights and biases\n",
        "            #of the following layer.\n",
        "            grads['dW' + str(i)] = dWtemp\n",
        "            grads['db' + str(i)] = dbtemp\n",
        "#        print(grads.keys())\n",
        "        return grads\n",
        "\n",
        "    #Now its time for gradient descent!!! Actually updating the weights and biases according to calculated\n",
        "    #grads and the learning rate, a hyperparameter.\n",
        "    '''\n",
        "    The learning rate hyperparameter exists to facilitate the size of steps in gradient descent.\n",
        "    If the learning rate is too small, convergence to a local minimum takes too long.\n",
        "    If the learning rate is too big, the process can overshoot and diverge, not reaching the minimum.\n",
        "    '''\n",
        "    #Also, we don't know when a minimum is reached, we just try to get close. More epochs exist to try\n",
        "    #and approach the minimum of the cost function.\n",
        "    '''\n",
        "    Many optimizers use dynamic learning rates, decreasing step size as a minimum seems to be approached\n",
        "    to get a more accurate estimate without overshooting.\n",
        "    '''\n",
        "    def updateParameters(self, grads, learningRate):\n",
        "        L = len(self.params) // 2 #layer number\n",
        "\n",
        "#        print(L)\n",
        "        for i in range(L):\n",
        "            #starting with first hidden layer updating current Weights\n",
        "            self.params['W'+str(i+1)] = self.params['W'+str(i+1)] - learningRate*grads['dW'+str(i)]\n",
        "            #then biases\n",
        "            self.params['b'+str(i+1)] = self.params['b'+str(i+1)] - learningRate*grads['db'+str(i)]\n",
        "\n",
        "            '''\n",
        "            W_new = W - lr*grad_b\n",
        "            b_new = b - lr*grad_b\n",
        "\n",
        "            What it looks like to me is a steeper negative gradient will increase a weight ot a bias\n",
        "            while a positive gradient value decreases the weight or the bias.\n",
        "\n",
        "            A positive gradient means that increasing that parameter would increase the cost\n",
        "            (if we consider partial derivative of the cost function with respect to weights or biases),\n",
        "            so we decrease a weight associated with a positive gradient. Negative gradients decrease the cost\n",
        "            so we increase those weights to emphasize them more.\n",
        "            '''\n",
        "\n",
        "    #Now the function must be trained.\n",
        "    X_TEST = None\n",
        "    Y_TEST = None\n",
        "    accHistory = []\n",
        "    lrHist = []\n",
        "    def train(self, X, y, layerDims, epochs, learningRate, runOptimizer = True):\n",
        "\n",
        "        params = self.initializeParams(layerDims)\n",
        "        costHistory = []\n",
        "        accSlopes = []\n",
        "\n",
        "        # One-hot encode labels on GPU\n",
        "        encoder = OneHotEncoder(sparse_output=False)\n",
        "        y_np = y.cpu().numpy().reshape(-1, 1) # Move to CPU for sklearn\n",
        "        y_encoded_np = encoder.fit_transform(y_np)\n",
        "        y_encoded = torch.from_numpy(y_encoded_np).float().cuda() # Convert back to tensor on GPU\n",
        "\n",
        "        # One-hot encode test labels on GPU\n",
        "        y_test_np = FNN_classifier.Y_TEST.cpu().numpy().reshape(-1, 1) # Move to CPU for sklearn\n",
        "        y_test_encoded_np = encoder.transform(y_test_np)\n",
        "        FNN_classifier.Y_TEST_ENCODED = torch.from_numpy(y_test_encoded_np).float().cuda() # Convert back to tensor on GPU\n",
        "\n",
        "\n",
        "        for i in range(epochs):\n",
        "            Y_hat, caches = self.forwardProp(X)\n",
        "            cost = self.crossEntropyLoss(Y_hat, y_encoded) # Use encoded training labels\n",
        "            costHistory.append(cost.item()) # Append scalar value\n",
        "\n",
        "            grads = self.backprop(Y_hat, y_encoded, caches) # Use encoded training labels\n",
        "\n",
        "            params = self.updateParameters(grads, learningRate)\n",
        "\n",
        "            accuracy = torch.mean((self.predict(FNN_classifier.X_TEST) == torch.argmax(FNN_classifier.Y_TEST_ENCODED, axis=1)).float()) # Compare with encoded test labels\n",
        "            self.accHistory.append(accuracy.item()) # Append scalar value\n",
        "\n",
        "            #Approximates derivative of the accHistory function\n",
        "            if len(self.accHistory) > 1:\n",
        "                accSlopes.append(self.accHistory[i]-self.accHistory[i-1])\n",
        "\n",
        "            #running my optimizer\n",
        "            if runOptimizer:\n",
        "                if len(accSlopes) >= 3:\n",
        "                    learningRate = self.updateLR(i, epochs, accSlopes, learningRate)\n",
        "                    self.lrHist.append(learningRate)\n",
        "\n",
        "            if i%10 == 0:\n",
        "                print(f\"Epoch {i}: Accuracy = {accuracy.item() * 100:.2f}% | LR = {learningRate}\") # Use .item() for printing\n",
        "\n",
        "        return params, costHistory\n",
        "\n",
        "    def predict(self, X):\n",
        "        A, _ = self.forwardProp(X)\n",
        "        return torch.argmax(A, axis=0) # Use torch.argmax\n",
        "\n",
        "    #My self-designed optimizer program inspired from RMS-prop.\n",
        "    def updateLR(self, i, epochs, accSlopes, learningRate):\n",
        "        first = accSlopes[0]\n",
        "        if first < 0:\n",
        "            first = 0 - first\n",
        "        lr = learningRate\n",
        "        ###recentSlopeAvg = (accSlopes[i] + accSlopes[i-1] + accSlopes[i-2])/3\n",
        "        #Optimization begins after certain fraction of allotted epochs is done to make sure stabilization has started.\n",
        "        #For index errors, stops before last epoch.\n",
        "        if epochs > 0 and i/epochs > 0.2 and i < epochs:\n",
        "            if accSlopes[i-1] < accSlopes[i-2] and accSlopes[i-1] > 0:\n",
        "                #decrease the learning rate proportionally to the derivative decrease but within limits 0.75 to 1.\n",
        "                changeFactor = accSlopes[i-1]/first\n",
        "                if changeFactor < 0.75:\n",
        "                    changeFactor = 0.75\n",
        "                if changeFactor > 1:\n",
        "                    changeFactor = 1\n",
        "                lr *= changeFactor\n",
        "            if accSlopes[i-1] < 0:\n",
        "                cslope = 0 - accSlopes[i-1]\n",
        "                changeFactor = max(cslope,first)/min(cslope,first) #ensures >1 value\n",
        "                if changeFactor < 1:\n",
        "                    changeFactor = 1\n",
        "                if changeFactor > 1.1:\n",
        "                    changeFactor = 1.1\n",
        "                lr *= changeFactor\n",
        "\n",
        "        return lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    -------------------------------------------------------------------------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    Changes to make:\n",
        "    - Implement softmax for the final layer in forward prop - DONE\n",
        "    - Create an optimizer system to dynamically change the learning rate\n",
        "    - Implement mini-batch gradient descent\n",
        "    - Number display\n",
        "    '''\n",
        "\n",
        "# Store test data as class attributes\n",
        "FNN_classifier.X_TEST = X_test\n",
        "FNN_classifier.Y_TEST = y_test # Store original test labels as well\n",
        "\n",
        "\n",
        "layerDims = [784,512,10] #1 hidden layer specified. 784*256 + 256*10 = 200704 + 2560 = 203264 total weights\n",
        "\n",
        "\n",
        "print(\"Training neural network...\")\n",
        "\n",
        "network = FNN_classifier()\n",
        "network.activation = \"relu\" #relu, tanh, sigmoid are available\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "params, costHistory = network.train(X_train, y_train, layerDims, epochs = 1000, learningRate = 1.2, runOptimizer=False)\n",
        "#use 0.6 learning rate for fashion mnist, 1.2 for mnist\n",
        "end = time.time() #times training\n",
        "print(f\"Training completed in {end - start} seconds.\")\n",
        "#Make plots to display mnist numbers and plot cost history\n",
        "\n",
        "\n",
        "\n",
        "print(\"Calculating final accuracy and graphing accuracy and cost histories...\")\n",
        "yPred = network.predict(X_test)\n",
        "\n",
        "#    print(\"Unique classes predicted:\", np.unique(yPred))\n",
        "print(\"First few predictions:\\t\", yPred[:35].cpu().numpy())\n",
        "print(\"Expected labels:\\t\", torch.argmax(FNN_classifier.Y_TEST_ENCODED, axis=1)[:35].cpu().numpy())\n",
        "#    print(yPred.shape)\n",
        "#    print(y_test.shape)\n",
        "accuracy = torch.mean((yPred == torch.argmax(FNN_classifier.Y_TEST_ENCODED, axis=1)).float())\n",
        "print(f\"Accuracy: {accuracy.item() * 100:.2f}%\")\n",
        "\n",
        "fig,axs = plt.subplots(2)\n",
        "fig.suptitle('Network Accuracy, Cost History')\n",
        "axs[0].plot(network.accHistory, label = 'Accuracy History', color = 'blue')\n",
        "axs[1].plot(costHistory, label = 'Cost History', color = 'red')\n",
        "#axs[2].plot(network.lrHist, label = 'LR History', color = 'green')\n",
        "fig.legend()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f3a9acbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46094cda-67ba-4e28-d8f2-30f59e2ac845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy result shape: (64, 32)\n",
            "CUDA kernel result shape: (64, 32)\n",
            "Are the results from NumPy and CUDA kernel close? True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Test with some sample matrices\n",
        "M, N, K = 64, 128, 32 # Choose some dimensions for testing\n",
        "\n",
        "A_np = np.random.rand(M, N).astype(np.float32)\n",
        "B_np = np.random.rand(N, K).astype(np.float32)\n",
        "\n",
        "# Perform matrix multiplication using numpy\n",
        "C_np = np.dot(A_np, B_np)\n",
        "\n",
        "# Perform matrix multiplication using your CUDA kernel\n",
        "A_cuda = torch.from_numpy(A_np).cuda()\n",
        "B_cuda = torch.from_numpy(B_np).cuda()\n",
        "C_cuda = torch.empty((M, K), dtype=torch.float32, device='cuda')\n",
        "\n",
        "matmul_module.matmul(A_cuda, B_cuda, C_cuda, M, N, K)\n",
        "C_cuda_np = C_cuda.cpu().numpy()\n",
        "\n",
        "# Compare the results\n",
        "tolerance = 1e-5\n",
        "are_close = np.allclose(C_np, C_cuda_np, atol=tolerance)\n",
        "\n",
        "print(f\"Numpy result shape: {C_np.shape}\")\n",
        "print(f\"CUDA kernel result shape: {C_cuda_np.shape}\")\n",
        "print(f\"Are the results from NumPy and CUDA kernel close? {are_close}\")\n",
        "\n",
        "if not are_close:\n",
        "    print(\"\\nDiscrepancies found. Let's look at a small section of the results:\")\n",
        "    print(\"NumPy result (first 5x5):\\n\", C_np[:5, :5])\n",
        "    print(\"CUDA kernel result (first 5x5):\\n\", C_cuda_np[:5, :5])\n",
        "    # You might want to print more of the matrices or specific differing elements for debugging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ORIGINAL NEURAL NETWORK ON CPU USING NUMPY\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "class FNN_classifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params = {}\n",
        "\n",
        "    def initializeParams(self, layerDims):\n",
        "        #initialize weights and biases of the network given array of dimensions of each layer\n",
        "        np.random.seed(42)\n",
        "        for i in range(1,len(layerDims)):\n",
        "            '''\n",
        "                W_i is a 2 dimensional matrix that is initialized to random values with\n",
        "                dimensions n*m where n is the layers in the current dimension and\n",
        "                m is the layers of the previous dimension. Each value is multiplied by 0.01\n",
        "                to avoid exploding gradients.\n",
        "                The shape of the n*m matrix allows it to be multiplied in the current layer in\n",
        "                forward propagation by the result of the previous layer which is an m-dimensional\n",
        "                vector (Matrix mult: n*m x m*1). This results in an n dimensional vector that can then\n",
        "                be taken to the next layer to repeat the process.\n",
        "\n",
        "                b_i is an n dimensional vector of zeros representing biases. biases don't start\n",
        "                with random values like weights do.\n",
        "\n",
        "                Multiplication structure for a 2 by 4 weight metrix to create the linear hypothesis\n",
        "                (m and n here are unrelated to previous m and n):\n",
        "\n",
        "                [a b c d]   [i]   [m]   [o]\n",
        "                [e f g h] X [j] + [n] = [p]\n",
        "                            [k]\n",
        "                            [l]\n",
        "            '''\n",
        "            self.params['W' + str(i)] = np.random.randn(layerDims[i],layerDims[i-1])*0.1\n",
        "            self.params['b' + str(i)] = np.zeros((layerDims[i],1))\n",
        "\n",
        "\n",
        "        return self.params\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    Activation functions\n",
        "    \"\"\"\n",
        "    #Default value, can be changed\n",
        "    activation = \"relu\"\n",
        "\n",
        "    #functions return the value of the function given input Z and returns Z for caching\n",
        "    def relu(self,Z):\n",
        "        return np.maximum(0,Z), Z\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return 1/(1+np.exp(-Z)), Z\n",
        "\n",
        "    def tanh(self,Z):\n",
        "        return np.tanh(Z), Z\n",
        "\n",
        "    def softmax(self,Z):\n",
        "        \"\"\"\n",
        "        ex. for a 3 element vector:\n",
        "        Z\n",
        "        \"\"\"\n",
        "        zexp = np.exp(Z - np.max(Z,axis=0,keepdims=True)) #Apparently this is a trick that prevents overflow\n",
        "                                    #This will automatically make it so that e^largest value = e^0 = 1\n",
        "                                    #This is also mathematically equivalent to regular softmax\n",
        "        return zexp/np.sum(zexp, axis=0, keepdims = True), Z\n",
        "\n",
        "    \"\"\"\n",
        "    ------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "    def forwardProp(self,X):\n",
        "        X = X.T\n",
        "        #X is the incoming input matrix\n",
        "        A = X\n",
        "        caches = [] #stores tuples of the linear and activation caches for each layer of the network\n",
        "\n",
        "        L = len(self.params)//2 #self.params is twice as long as the number of layers due to W_i and b_i\n",
        "\n",
        "        for i in range(1, L+1):\n",
        "            prevA = A\n",
        "\n",
        "            #Z = Wx + b - \"linear hypothesis\"\n",
        "\n",
        "            Z = np.dot(self.params['W' + str(i)],prevA) + self.params['b' + str(i)]\n",
        "\n",
        "            #Caching important values from linear hypothesis\n",
        "            linearCache = (prevA, self.params['W' + str(i)], self.params['b' + str(i)])\n",
        "\n",
        "            #Caching important values from activation. Activation function returns the\n",
        "            #value of the function itself and the input Z, the result of the linear hypothesis\n",
        "\n",
        "            func = 0\n",
        "            if i == L: #This is the final layer, where softmax is implemented.\n",
        "                func = self.softmax(Z)\n",
        "                #print(f\"final layer reached, softmaxed. {L}\")\n",
        "            elif self.activation == \"relu\":\n",
        "                #print(f\"relu'd{L}\")\n",
        "                func = self.relu(Z)\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                func = self.sigmoid(Z)\n",
        "            elif self.activation == \"tanh\":\n",
        "                func = self.tanh(Z)\n",
        "\n",
        "            A, activationCache = func\n",
        "\n",
        "            cache = (linearCache,activationCache)\n",
        "            caches.append(cache)\n",
        "\n",
        "        return A, caches\n",
        "\n",
        "\n",
        "    #Now that forward propagation is done, the loss function needs to be defined\n",
        "    def crossEntropyLoss(self, yTrue, yPred):\n",
        "        yTrue = yTrue.T\n",
        "        \"\"\"\n",
        "        Computes cost function for general cross entropy loss, used for multiclass classification\n",
        "        \"\"\"\n",
        "        #accounts for floating point inaccuracies, keeps values between 0 and 1\n",
        "        yPred = np.clip(yPred, 1e-15, 1-1e-15)\n",
        "\n",
        "        #returns a vector of the individual sample losses\n",
        "\n",
        "        losses = -np.sum(yTrue*np.log(yPred),axis=0)\n",
        "\n",
        "        #Determines total cost by taking the average of the loss across the entire dataset.\n",
        "        cost = np.mean(losses)\n",
        "        return cost\n",
        "\n",
        "\n",
        "    #Backprop helper function that runs it for a single layer\n",
        "    def stepBack(self, dA, cache, isOutput = False):\n",
        "        #dA is the gradient of the loss with respect to A, dL/dA\n",
        "        #unpacking cache\n",
        "        linearCache, activationCache = cache\n",
        "        prevA, W, b = linearCache\n",
        "        Z = activationCache\n",
        "        m = prevA.shape[1] #gets number of samples\n",
        "\n",
        "\n",
        "        if isOutput:\n",
        "            dZ = dA\n",
        "        else:\n",
        "            if self.activation == \"relu\":\n",
        "                dZ = dA * (Z>0) #Interestingly enough, you can multiply numbers by booleans in python.\n",
        "                #This multiplies dA by the derivative of relu which is 1 if positive and 0 if not.\n",
        "            elif self.activation == \"tanh\":\n",
        "                dZ = dA * (1-np.tanh(Z)**2)\n",
        "                #for y = tanh(x), dy/dx = 1-tanh(x)^2\n",
        "            elif self.activation == \"sigmoid\":\n",
        "                dZ = dA*self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "                #for y = sigmoid(x), dy/dx = sigmoid(x)*(1-sigmoid(x))\n",
        "            else:\n",
        "                raise ValueError(\"Activation function not supported\")\n",
        "\n",
        "        #Now, we can compute gradients for W, b, and prevA\n",
        "        dW = (1/m)*np.dot(dZ,prevA.T)\n",
        "        db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
        "        prevdA = np.dot(W.T, dZ)\n",
        "\n",
        "        return prevdA, dW, db\n",
        "\n",
        "    #Now that the stepback function is defined for each layer going backwards, we can do backprop\n",
        "    def backprop(self, AL, yPred, caches):\n",
        "        grads = {}\n",
        "        L = len(caches)\n",
        "\n",
        "        m = AL.shape[1]\n",
        "        #Remember, AL is the final activations for every sample represented as a matrix of column vectors\n",
        "        '''\n",
        "        ex.      [p0a p0b p0c p0d p0e]\n",
        "                 [p1a p1b p1c p1d p1e]\n",
        "                 [p2a p2b p2c p2d p2e]\n",
        "                 [p3a p3b p3c p3d p3e]\n",
        "                 [p4a p4b p4c p4d p4e]\n",
        "        Where each final letter of a term represents a different sample. AL.shape[1] returns the number of columns,\n",
        "        also the number of samples.\n",
        "        '''\n",
        "        yPred = yPred.T\n",
        "\n",
        "        dAL = AL - yPred\n",
        "        #Gradient of the loss function with respect to AL for general cross entropy loss.\n",
        "\n",
        "        currentCache = caches[L-1]\n",
        "        grads['dA' + str(L-1)], grads['dW' + str(L-1)], grads['db' + str(L-1)] = self.stepBack(dAL, currentCache, isOutput=True)\n",
        "\n",
        "        for i in reversed(range(L-1)):\n",
        "            currentCache = caches[i]\n",
        "            prevdAtemp, dWtemp, dbtemp = self.stepBack(grads['dA' + str(i+1)], currentCache)\n",
        "\n",
        "            grads['dA' + str(i)] = prevdAtemp\n",
        "            #i+1 is used in order to match up the dA gradient used to calculate the weights and biases\n",
        "            #of the following layer.\n",
        "            grads['dW' + str(i)] = dWtemp\n",
        "            grads['db' + str(i)] = dbtemp\n",
        "#        print(grads.keys())\n",
        "        return grads\n",
        "\n",
        "    #Now its time for gradient descent!!! Actually updating the weights and biases according to calculated\n",
        "    #grads and the learning rate, a hyperparameter.\n",
        "    '''\n",
        "    The learning rate hyperparameter exists to facilitate the size of steps in gradient descent.\n",
        "    If the learning rate is too small, convergence to a local minimum takes too long.\n",
        "    If the learning rate is too big, the process can overshoot and diverge, not reaching the minimum.\n",
        "    '''\n",
        "    #Also, we don't know when a minimum is reached, we just try to get close. More epochs exist to try\n",
        "    #and approach the minimum of the cost function.\n",
        "    '''\n",
        "    Many optimizers use dynamic learning rates, decreasing step size as a minimum seems to be approached\n",
        "    to get a more accurate estimate without overshooting.\n",
        "    '''\n",
        "    def updateParameters(self, grads, learningRate):\n",
        "        L = len(self.params) // 2 #layer number\n",
        "\n",
        "#        print(L)\n",
        "        for i in range(L):\n",
        "            #starting with first hidden layer updating current Weights\n",
        "\n",
        "\n",
        "\n",
        "            self.params['W'+str(i+1)] = self.params['W'+str(i+1)] - learningRate*grads['dW'+str(i)]\n",
        "            #then biases\n",
        "            self.params['b'+str(i+1)] = self.params['b'+str(i+1)] - learningRate*grads['db'+str(i)]\n",
        "\n",
        "            '''\n",
        "            W_new = W - lr*grad_b\n",
        "            b_new = b - lr*grad_b\n",
        "\n",
        "            What it looks like to me is a steeper negative gradient will increase a weight ot a bias\n",
        "            while a positive gradient value decreases the weight or the bias.\n",
        "\n",
        "            A positive gradient means that increasing that parameter would increase the cost\n",
        "            (if we consider partial derivative of the cost function with respect to weights or biases),\n",
        "            so we decrease a weight associated with a positive gradient. Negative gradients decrease the cost\n",
        "            so we increase those weights to emphasize them more.\n",
        "            '''\n",
        "\n",
        "    #Now the function must be trained.\n",
        "    X_TEST = None\n",
        "    Y_TEST = None\n",
        "    accHistory = []\n",
        "    lrHist = []\n",
        "    def train(self, X, y, layerDims, epochs, learningRate, runOptimizer = True):\n",
        "\n",
        "        params = self.initializeParams(layerDims)\n",
        "        costHistory = []\n",
        "        accSlopes = []\n",
        "        for i in range(epochs):\n",
        "            Y_hat, caches = self.forwardProp(X)\n",
        "            cost = self.crossEntropyLoss(Y_hat,y)\n",
        "            costHistory.append(cost)\n",
        "            grads = self.backprop(Y_hat, y, caches)\n",
        "\n",
        "            params = self.updateParameters(grads, learningRate)\n",
        "\n",
        "            accuracy = np.mean(self.predict(FNN_classifier.X_TEST) == np.argmax(FNN_classifier.Y_TEST, axis=1))\n",
        "            self.accHistory.append(accuracy)\n",
        "\n",
        "            #Approximates derivative of the accHistory function\n",
        "            if len(self.accHistory) > 1:\n",
        "                accSlopes.append(self.accHistory[i]-self.accHistory[i-1])\n",
        "\n",
        "            #running my optimizer\n",
        "            if runOptimizer:\n",
        "                if len(accSlopes) >= 3:\n",
        "                    learningRate = self.updateLR(i, epochs, accSlopes, learningRate)\n",
        "                    self.lrHist.append(learningRate)\n",
        "\n",
        "            if i%2 == 0:\n",
        "                print(f\"Epoch {i}: Accuracy = {accuracy * 100:.2f}% | LR = {learningRate}\")\n",
        "\n",
        "        return params, costHistory\n",
        "\n",
        "    def predict(self, X):\n",
        "        A, _ = self.forwardProp(X)\n",
        "#        print(A.shape)\n",
        "        return np.argmax(A, axis=0) #return highest probability index\n",
        "\n",
        "    #My self-designed optimizer program inspired from RMS-prop.\n",
        "    def updateLR(self, i, epochs, accSlopes, learningRate):\n",
        "        first = accSlopes[0]\n",
        "        if first < 0:\n",
        "            first = 0 - first\n",
        "        lr = learningRate\n",
        "        ###recentSlopeAvg = (accSlopes[i] + accSlopes[i-1] + accSlopes[i-2])/3\n",
        "        #Optimization begins after certain fraction of allotted epochs is done to make sure stabilization has started.\n",
        "        #For index errors, stops before last epoch.\n",
        "        if epochs > 0 and i/epochs > 0.2 and i < epochs:\n",
        "            if accSlopes[i-1] < accSlopes[i-2] and accSlopes[i-1] > 0:\n",
        "                #decrease the learning rate proportionally to the derivative decrease but within limits 0.75 to 1.\n",
        "                changeFactor = accSlopes[i-1]/first\n",
        "                if changeFactor < 0.75:\n",
        "                    changeFactor = 0.75\n",
        "                if changeFactor > 1:\n",
        "                    changeFactor = 1\n",
        "                lr *= changeFactor\n",
        "            if accSlopes[i-1] < 0:\n",
        "                cslope = 0 - accSlopes[i-1]\n",
        "                changeFactor = max(cslope,first)/min(cslope,first) #ensures >1 value\n",
        "                if changeFactor < 1:\n",
        "                    changeFactor = 1\n",
        "                if changeFactor > 1.1:\n",
        "                    changeFactor = 1.1\n",
        "                lr *= changeFactor\n",
        "\n",
        "        return lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    -------------------------------------------------------------------------------------------------------\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    Changes to make:\n",
        "    - Implement softmax for the final layer in forward prop - DONE\n",
        "    - Create an optimizer system to dynamically change the learning rate\n",
        "    - Implement mini-batch gradient descent\n",
        "    - Number display\n",
        "    '''\n",
        "\n",
        "#get mnist\n",
        "print(\"Loading MNIST dataset...\")\n",
        "mnist = fetch_openml('Fashion-MNIST', version=1)\n",
        "X,y = mnist['data'], mnist['target']\n",
        "\n",
        "print(\"Encoding data...\")\n",
        "\n",
        "y = y.astype(int)\n",
        "X = X/255.0\n",
        "\n",
        "#one-hot encode labels in y\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(np.array(y).reshape(-1,1)) #y.shape = (70000, 10)\n",
        "#X = X.T #Input format is n*m where m is samples, and current format is 70000*784. Changes to 784*70000\n",
        "\n",
        "\n",
        "print(\"Splitting data...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
        "\n",
        "FNN_classifier.X_TEST = X_test\n",
        "FNN_classifier.Y_TEST = y_test\n",
        "\n",
        "\n",
        "\n",
        "layerDims = [784,512,10] #1 hidden layer specified. 784*256 + 256*10 = 200704 + 2560 = 203264 total weights\n",
        "\n",
        "\n",
        "print(\"Training neural network...\")\n",
        "\n",
        "network = FNN_classifier()\n",
        "network.activation = \"relu\" #relu, tanh, sigmoid are available\n",
        "\n",
        "start = time.time()\n",
        "params, costHistory = network.train(X_train, y_train, layerDims, epochs = 1000, learningRate = 0.6, runOptimizer=False)\n",
        "#use 0.6 learning rate for fashion mnist, 1.2 for mnist\n",
        "end = time.time() #times training\n",
        "print(f\"Training completed in {end - start} seconds.\")\n",
        "#Make plots to display mnist numbers and plot cost history\n",
        "\n",
        "\n",
        "\n",
        "print(\"Calculating final accuracy and graphing accuracy and cost histories...\")\n",
        "yPred = network.predict(X_test)\n",
        "\n",
        "#    print(\"Unique classes predicted:\", np.unique(yPred))\n",
        "print(\"First few predictions:\\t\", yPred[:35])\n",
        "print(\"Expected labels:\\t\", np.argmax(y_test, axis=1)[:35])\n",
        "#    print(yPred.shape)\n",
        "#    print(y_test.shape)\n",
        "accuracy = np.mean(yPred == np.argmax(y_test, axis=1))\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "fig,axs = plt.subplots(3)\n",
        "fig.suptitle('Network Accuracy, Cost Histories and LR History')\n",
        "axs[0].plot(network.accHistory, label = 'Accuracy History', color = 'blue')\n",
        "axs[1].plot(costHistory, label = 'Cost History', color = 'red')\n",
        "axs[2].plot(network.lrHist, label = 'LR History', color = 'green')\n",
        "fig.legend()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(10)\n",
        "\n",
        "#display mnist digits - WIP\n",
        "'''for i in range(5):\n",
        "    import matplotlib.pyplot as plt2\n",
        "    fig2, axs2 = plt2.subplots(5)\n",
        "    plt2.imshow(X_test.to_numpy()[i].reshape(28,28), cmap='gray')\n",
        "    plt2.title(f'Label: {y_test[i]}')\n",
        "'''\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zrdb_eVpqqhH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "a09dceaf-393c-4f60-f474-1bde2475dbba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:110: UserWarning: A network error occurred while downloading https://api.openml.org/data/v1/download/18238735. Retrying...\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-693127979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;31m#get mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading MNIST dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fashion-MNIST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;31m# obtain the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATA_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m     bunch = _download_data_to_bunch(\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mreturn_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mno_retry_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     X, y, frame, categories = _retry_with_clean_cache(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_retry_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_arff_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0moutput_array_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \"\"\"\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0mgzip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_openml_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_retries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_open_openml_url\u001b[0;34m(openml_path, data_home, n_retries, delay)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 with closing(\n\u001b[0;32m--> 173\u001b[0;31m                     _retry_on_network_error(n_retries, delay, req.full_url)(urlopen)(\n\u001b[0m\u001b[1;32m    174\u001b[0m                         \u001b[0mreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;31m# 412 is a specific OpenML error code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvMRfsmqhtIi6mSkLwCdp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}